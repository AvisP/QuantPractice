{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spell.ml/blog/pytorch-quantization-X8e7wBAAACIAHPhT\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan-zhihu-com.translate.goog/p/299108528?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=wapp\n",
    "\n",
    "https://zhuanlan-zhihu-com.translate.goog/p/149659607?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=wapp\n",
    "\n",
    "https://github.com/IntelLabs/distiller/issues/327 -> Quantizing a MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchvision\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets\n",
    "# import torchvision.transforms as transforms\n",
    "# import os\n",
    "# import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "# from loss import TripletLoss\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image as Img\n",
    "torch.set_printoptions(precision=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST 1\n",
    "##### No calibration with Input Images and weight set to +1 or -1 and bias only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "            super(M, self).__init__()\n",
    "            # QuantStub converts tensors from floating point to quantized\n",
    "            self.quant = torch.quantization.QuantStub()\n",
    "            # self.BN = nn.BatchNorm2d(3)\n",
    "            # self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "            # self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "            # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.03307433053851127625, -0.13484150171279907227, -0.21625524759292602539], \n",
    "            #                                                         [ 0.14247404038906097412, -0.14247404038906097412, -0.24932956695556640625], \n",
    "            #                                                         [ 0.32311078906059265137, -0.14501821994781494141, -0.21371106803417205811]]]]))\n",
    "            self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 1.0, 1.0, -1.0], \n",
    "                                                                    [ -1.0, 1.0, 1.0], \n",
    "                                                                    [ 1.0, 1.0, -1.0]]]]))\n",
    "            self.conv.bias = torch.nn.Parameter(torch.tensor([10.0]))\n",
    "            # self.relu = torch.nn.ReLU()\n",
    "            # DeQuantStub converts tensors from quantized to floating point\n",
    "            self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.quant(x)\n",
    "        # x = self.BN(x)\n",
    "        x = self.conv(x)\n",
    "        # x = self.relu(x)      \n",
    "        x = self.dequant(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fp32.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.0, 0.0, 0.0],\n",
    "#           [ 0.0,  1.0, 0.0],\n",
    "#           [ 0.0, 0.0, 0.0]]]]))\n",
    "# model_fp32.conv.bias = torch.nn.Parameter(torch.tensor([0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fp32.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.0, 0.0, 0.0],\n",
    "#           [ 0.0,  1.0, 0.0],\n",
    "#           [ 0.0, 0.0, 0.0]]]]))\n",
    "# model_fp32.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.03307433053851127625, -0.13484150171279907227, -0.21625524759292602539], \n",
    "# [ 0.14247404038906097412, -0.14247404038906097412, -0.24932956695556640625], \n",
    "# [ 0.32311078906059265137, -0.14501821994781494141, -0.21371106803417205811]]]]))\n",
    "# model_fp32.conv.bias = torch.nn.Parameter(torch.tensor([0.1095]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:172: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1107: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_fp32 = M()\n",
    "model_fp32.eval()\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, [['conv', 'relu']])\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, ['conv'])\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32)\n",
    "model_fp32_converted = torch.quantization.convert(model_fp32_prepared, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32.qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.quant.zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.quant.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "            -0.996078491210937500000000000000],\n",
       "           [-0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "             0.996078491210937500000000000000],\n",
       "           [ 0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "            -0.996078491210937500000000000000]]]], size=(1, 1, 3, 3),\n",
       "        dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
       "        scale=tensor([0.007843137718737125396728515625], dtype=torch.float64),\n",
       "        zero_point=tensor([0]), axis=0),\n",
       " Parameter containing:\n",
       " tensor([10.], requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.conv._weight_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.conv.zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.conv.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (conv): QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "           -0.996078491210937500000000000000],\n",
       "          [-0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "            0.996078491210937500000000000000],\n",
       "          [ 0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "           -0.996078491210937500000000000000]]]], size=(1, 1, 3, 3),\n",
       "       dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
       "       scale=tensor([0.007843137718737125396728515625], dtype=torch.float64),\n",
       "       zero_point=tensor([0]), axis=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.conv.weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "            -0.996078491210937500000000000000],\n",
       "           [-0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "             0.996078491210937500000000000000],\n",
       "           [ 0.996078491210937500000000000000,  0.996078491210937500000000000000,\n",
       "            -0.996078491210937500000000000000]]]], size=(1, 1, 3, 3),\n",
       "        dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
       "        scale=tensor([0.007843137718737125396728515625], dtype=torch.float64),\n",
       "        zero_point=tensor([0]), axis=0),\n",
       " Parameter containing:\n",
       " tensor([110.], requires_grad=True))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.conv._weight_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Conv2d.bias of QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeQuantize()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp32 = torch.tensor([[[[ 0.95466703176498413086, -1.36212718486785888672,\n",
    "            0.75253891944885253906,  1.57104063034057617188],\n",
    "          [ 0.97250884771347045898, -0.67004448175430297852,\n",
    "           -0.58047348260879516602,  1.30683445930480957031],\n",
    "          [-0.13423979282379150391,  0.16391958296298980713,\n",
    "           -0.71688455343246459961,  0.05846109613776206970],\n",
    "          [ 1.07569837570190429688, -0.06351475417613983154,\n",
    "           -0.19469638168811798096, -0.09430617839097976685]]]], requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.362127184867858886718750000000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.571040630340576171875000000000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.011502618901431560516357421875)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_fp32.max() - input_fp32.min())/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[13., 12., 13., 13.],\n",
       "          [13., 10., 11., 15.],\n",
       "          [13., 13., 10., 12.],\n",
       "          [12., 10., 11., 11.]]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantization algorithm calibration happens here\n",
    "# this example uses just a single sample, but obvious in prod you will\n",
    "# want to use some meaningful subset of your training or test set\n",
    "# instead.\n",
    "# input_fp32 = torch.randn(2, 3, 4, 4)\n",
    "# input_fp32 = torch.randn(1, 1, 4, 4)\n",
    "\n",
    "model_fp32_converted(input_fp32)\n",
    "\n",
    "# model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "# res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 2\n",
    "##### Weight set to 1 at center and bias is 0. This will pass in the image as it is. So output will be equal to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "            super(M1, self).__init__()\n",
    "            # QuantStub converts tensors from floating point to quantized\n",
    "            self.quant = torch.quantization.QuantStub()\n",
    "            # self.BN = nn.BatchNorm2d(3)\n",
    "            # self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "            # self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "            # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.03307433053851127625, -0.13484150171279907227, -0.21625524759292602539], \n",
    "            #                                                         [ 0.14247404038906097412, -0.14247404038906097412, -0.24932956695556640625], \n",
    "            #                                                         [ 0.32311078906059265137, -0.14501821994781494141, -0.21371106803417205811]]]]))\n",
    "            self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0., 0., 0.],\n",
    "                                                                    [0., 1., 0.],\n",
    "                                                                    [0., 0., 0.]]]]), requires_grad=False)\n",
    "            self.conv.bias = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "            # self.relu = torch.nn.ReLU()\n",
    "            # DeQuantStub converts tensors from quantized to floating point\n",
    "            self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.quant(x)\n",
    "        # x = self.BN(x)\n",
    "        x = self.conv(x)\n",
    "        # x = self.relu(x)      \n",
    "        x = self.dequant(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
     ]
    }
   ],
   "source": [
    "model_fp32_1 = M1()\n",
    "model_fp32_1.eval()\n",
    "model_fp32_1.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, [['conv', 'relu']])\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, ['conv'])\n",
    "model_fp32_prepared_1 = torch.quantization.prepare(model_fp32_1)\n",
    "model_fp32_prepared_1(input_fp32)   ### Passing the input through model before conversion for calibration\n",
    "model_fp32_converted_1 = torch.quantization.convert(model_fp32_prepared_1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_1.qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quantize(scale=tensor([0.019509643316268920898437500000]), zero_point=tensor([49]), dtype=torch.quint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_1.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.954667031764984130859375000000, -1.362127184867858886718750000000,\n",
       "            0.752538919448852539062500000000,  1.571040630340576171875000000000],\n",
       "          [ 0.972508847713470458984375000000, -0.670044481754302978515625000000,\n",
       "           -0.580473482608795166015625000000,  1.306834459304809570312500000000],\n",
       "          [-0.134239792823791503906250000000,  0.163919582962989807128906250000,\n",
       "           -0.716884553432464599609375000000,  0.058461096137762069702148437500],\n",
       "          [ 1.075698375701904296875000000000, -0.063514754176139831542968750000,\n",
       "           -0.194696381688117980957031250000, -0.094306178390979766845703125000]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.955972552299499511718750000000, -0.955972552299499511718750000000,\n",
       "            0.760876059532165527343750000000,  1.580281138420104980468750000000],\n",
       "          [ 0.975482165813446044921875000000, -0.663327872753143310546875000000,\n",
       "           -0.585289299488067626953125000000,  1.307146072387695312500000000000],\n",
       "          [-0.136567503213882446289062500000,  0.156077146530151367187500000000,\n",
       "           -0.721856832504272460937500000000,  0.058528929948806762695312500000],\n",
       "          [ 1.073030352592468261718750000000, -0.058528929948806762695312500000,\n",
       "           -0.195096433162689208984375000000, -0.097548216581344604492187500000]]]],\n",
       "       size=(1, 1, 4, 4), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.01950964331626892,\n",
       "       zero_point=49)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_1.quant(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), scale=0.01950964331626892, zero_point=49, padding=(1, 1))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_1.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_1.conv.zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01950964331626892"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_1.conv.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.955972552299499511718750000000, -0.955972552299499511718750000000,\n",
       "            0.760876059532165527343750000000,  1.580281138420104980468750000000],\n",
       "          [ 0.975482165813446044921875000000, -0.663327872753143310546875000000,\n",
       "           -0.585289299488067626953125000000,  1.307146072387695312500000000000],\n",
       "          [-0.136567503213882446289062500000,  0.156077146530151367187500000000,\n",
       "           -0.721856832504272460937500000000,  0.058528929948806762695312500000],\n",
       "          [ 1.073030352592468261718750000000, -0.058528929948806762695312500000,\n",
       "           -0.195096433162689208984375000000, -0.097548216581344604492187500000]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_1(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "            0.000000000000000000000000000000],\n",
       "           [0.000000000000000000000000000000, 0.996078491210937500000000000000,\n",
       "            0.000000000000000000000000000000],\n",
       "           [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "            0.000000000000000000000000000000]]]], size=(1, 1, 3, 3),\n",
       "        dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
       "        scale=tensor([0.007843137718737125396728515625], dtype=torch.float64),\n",
       "        zero_point=tensor([0]), axis=0),\n",
       " Parameter containing:\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_1.conv._weight_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Conv2d.bias of QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), scale=0.01950964331626892, zero_point=49, padding=(1, 1))>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_1.conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.03294463083148002625, -0.13431271910667419434,\n",
       "           -0.21540719270706176758],\n",
       "          [ 0.14191532135009765625, -0.14191532135009765625,\n",
       "           -0.24835182726383209229],\n",
       "          [ 0.32184368371963500977, -0.14444953203201293945,\n",
       "           -0.21287299692630767822]]]], size=(1, 1, 3, 3), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_channel_affine,\n",
       "       scale=tensor([0.00253420230001211166], dtype=torch.float64),\n",
       "       zero_point=tensor([0]), axis=0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted.conv.weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.10949999839067459106], requires_grad=True)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32.conv.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "            super(M, self).__init__()\n",
    "            # QuantStub converts tensors from floating point to quantized\n",
    "            # self.BN = nn.BatchNorm2d(3)\n",
    "            # self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "            # self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "            # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.03307433053851127625, -0.13484150171279907227, -0.21625524759292602539], \n",
    "            #                                                         [ 0.14247404038906097412, -0.14247404038906097412, -0.24932956695556640625], \n",
    "            #                                                         [ 0.32311078906059265137, -0.14501821994781494141, -0.21371106803417205811]]]]))\n",
    "            # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 1.0, 1.0, -1.0], \n",
    "            #                                                         [ -1.0, 1.0, 1.0], \n",
    "            #                                                         [ 1.0, 1.0, -1.0]]]]).double())\n",
    "            self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ -1.0, 0.0, 1.0], \n",
    "                                                                    [ -1.0, 0.0, 1.0], \n",
    "                                                                    [ -1.0, 0.0, 1.0]]]]).double())\n",
    "            self.conv.bias = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "            # self.relu = torch.nn.ReLU()\n",
    "            # DeQuantStub converts tensors from quantized to floating point\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp32 = torch.tensor([[[[ 0.95466703176498413086, -1.36212718486785888672,\n",
    "            0.75253891944885253906,  1.57104063034057617188],\n",
    "          [ 0.97250884771347045898, -0.67004448175430297852,\n",
    "           -0.58047348260879516602,  1.30683445930480957031],\n",
    "          [-0.13423979282379150391,  0.16391958296298980713,\n",
    "           -0.71688455343246459961,  0.05846109613776206970],\n",
    "          [ 1.07569837570190429688, -0.06351475417613983154,\n",
    "           -0.19469638168811798096, -0.09430617839097976685]]]], requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32 = M()\n",
    "model_fp32.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_2D(input_image: np.array, kernel: np.array) -> np.array:\n",
    "    k = kernel.shape[0]\n",
    "    tgt_size = input_image.shape[0]\n",
    "    padded_image = np.zeros(shape=(tgt_size+int((k+1)/2), tgt_size+int((k+1)/2)))\n",
    "    padded_image[1:input_image.shape[0]+1,1:input_image.shape[1]+1] = input_image\n",
    "    convolved_img = np.zeros(shape=(tgt_size, tgt_size))\n",
    "    for i in range(tgt_size):\n",
    "        for j in range(tgt_size):\n",
    "            mat = padded_image[i:i+k, j:j+k, :]\n",
    "            convolved_img[i, j] = np.sum(np.multiply(mat, kernel))\n",
    "    return convolved_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(input_image: np.array, kernel: np.array, bias: np.array) -> np.array:    \n",
    "    ### Add description of what dimension is what\n",
    "    k = kernel.shape[0]\n",
    "    number_input_row = input_image.shape[0]\n",
    "    number_input_col = input_image.shape[1]\n",
    "    input_depth = input_image.shape[2]   # R, G, B channel\n",
    "    filter_depth = kernel.shape[2]       # filter depth\n",
    "    filter_num = kernel.shape[3]         # number of filters\n",
    "    bias_depth = bias.shape[0]\n",
    "\n",
    "    if input_depth != filter_depth:\n",
    "        print(\"Error: Number of channels in both image and filter depth must match.\")\n",
    "        sys.exit()\n",
    "\n",
    "    if bias_depth != filter_num:\n",
    "        print(\"Error: Bias depth and filter number must match.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    if np.isnan(input_image):\n",
    "        print(\"Input image has NaN values\")\n",
    "        sys.exit()\n",
    "\n",
    "    if np.isnan(kernel):\n",
    "        print(\"Kernel has NaN values\")\n",
    "        sys.exit()\n",
    "\n",
    "    padded_image = np.zeros(shape=(number_input_row+int((k+1)/2), number_input_col+int((k+1)/2), input_depth))\n",
    "    padded_image[((k-1)>>1):number_input_row + ((k-1)>>1), ((k-1)>>1):number_input_col + ((k-1)>>1), :] = input_image\n",
    "    convolved_img = np.zeros(shape=(number_input_row, number_input_col, filter_num))\n",
    "\n",
    "    for f in range(filter_num):\n",
    "        for i in range(number_input_row):\n",
    "            for j in range(number_input_col):\n",
    "                mat = padded_image[i:i+k, j:j+k, :]\n",
    "                convolved_img[i, j, f ] = np.sum(np.multiply(mat, kernel[:,:,:,f]))\n",
    "        \n",
    "        convolved_img[:, :, f ] += bias[f]\n",
    "\n",
    "\n",
    "    if np.isnan(convolved_img):\n",
    "        print(\"convolved Image has NaN values\")\n",
    "        sys.exit()\n",
    "\n",
    "    return convolved_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter = np.zeros((5, 5, 3, 2))\n",
    "\n",
    "filter = np.zeros((3, 3, 3, 2))\n",
    "filter[:, :, 0, 0] = np.array([[[-1, 0, 1], \n",
    "                                   [-1, 0, 1], \n",
    "                                   [-1, 0, 1]]])\n",
    "filter[:, :, 1, 0] = np.array([[[1,   1,  1], \n",
    "                                   [0,   0,  0], \n",
    "                                   [-1, -1, -1]]])\n",
    "filter[:, :, 2, 0] = np.array([[[1,   1,  1], \n",
    "                                   [0,  1,  0], \n",
    "                                   [-1, -1, -1]]])\n",
    "\n",
    "filter[:, :, 0, 1] = np.array([[[-1, 0, 1], \n",
    "                                   [-1, 0, 1], \n",
    "                                   [-1, 0, 1]]])\n",
    "filter[:, :, 1, 1] = np.array([[[1,   1,  1], \n",
    "                                   [0,   0,  0], \n",
    "                                   [-1, -1, -1]]])\n",
    "filter[:, :, 2, 1] = np.array([[[1,   1,  1], \n",
    "                                   [0,   1,  0], \n",
    "                                   [-1, -1, -1]]])\n",
    "kernel = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "input_image = skimage.data.chelsea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 451, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 451, 2)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 2)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-66.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.multiply(mat, kernel[:,:,:,f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 451)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32.conv.weight.detach().numpy()[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.,  0.,  1.],\n",
       "         [-1.,  0.,  1.],\n",
       "         [-1.,  0.,  1.]]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32.conv.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (300,451) into shape (301,301)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-8969c22da6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvolved_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fp32\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-5e04aa3a41ba>\u001b[0m in \u001b[0;36mconvolve\u001b[0;34m(img, kernel)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtgt_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#-(k-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpadded_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpadded_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mconvolved_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (300,451) into shape (301,301)"
     ]
    }
   ],
   "source": [
    "convolved_img = convolve(img, model_fp32.conv.weight.detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03334471, -0.03334471, -0.00560314, ...,  0.1419949 ,\n",
       "         0.07033529, -0.01593922],\n",
       "       [-0.04118784, -0.03334471, -0.00980745, ...,  0.09355216,\n",
       "         0.02522588, -0.04928392],\n",
       "       [-0.04175333, -0.02998863, -0.01401176, ...,  0.05882353,\n",
       "         0.00561804, -0.04144078],\n",
       "       ...,\n",
       "       [ 0.2025298 ,  0.23276353,  0.15765059, ..., -0.04734941,\n",
       "        -0.16304627, -0.07788784],\n",
       "       [ 0.21147412,  0.14478471,  0.0173302 , ..., -0.02637255,\n",
       "        -0.14599098, -0.08239765],\n",
       "       [ 0.10724353, -0.0165651 , -0.1733902 , ..., -0.00058824,\n",
       "        -0.11854   , -0.09024078]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.032171666622161865234375000000, -1.755110442638397216796875000000,\n",
       "            4.910046756267547607421875000000, -0.172065436840057373046875000000],\n",
       "          [-1.868252083659172058105468750000, -2.337755203247070312500000000000,\n",
       "            4.804588269442319869995117187500,  0.544819116592407226562500000000],\n",
       "          [-0.569639652967453002929687500000, -3.406021848320960998535156250000,\n",
       "            1.840629030019044876098632812500,  1.492054417729377746582031250000],\n",
       "          [ 0.100404828786849975585937500000, -1.853039517998695373535156250000,\n",
       "           -0.136249911040067672729492187500,  0.911580935120582580566406250000]]]],\n",
       "       dtype=torch.float64, grad_fn=<SlowConv2DBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32(input_fp32.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32[0][0].detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.954667031764984130859375000000, -1.362127184867858886718750000000,\n",
       "            0.752538919448852539062500000000,  1.571040630340576171875000000000],\n",
       "          [ 0.972508847713470458984375000000, -0.670044481754302978515625000000,\n",
       "           -0.580473482608795166015625000000,  1.306834459304809570312500000000],\n",
       "          [-0.134239792823791503906250000000,  0.163919582962989807128906250000,\n",
       "           -0.716884553432464599609375000000,  0.058461096137762069702148437500],\n",
       "          [ 1.075698375701904296875000000000, -0.063514754176139831542968750000,\n",
       "           -0.194696381688117980957031250000, -0.094306178390979766845703125000]]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.95466703, -1.3621272 ,  0.7525389 ,  1.5710406 ],\n",
       "         [ 0.97250885, -0.6700445 , -0.5804735 ,  1.3068345 ],\n",
       "         [-0.1342398 ,  0.16391958, -0.71688455,  0.0584611 ],\n",
       "         [ 1.0756984 , -0.06351475, -0.19469638, -0.09430618]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.032171666622161865234375000000, -1.755110442638397216796875000000,\n",
       "            4.910046756267547607421875000000, -0.172065436840057373046875000000],\n",
       "          [-1.868252083659172058105468750000, -2.337755203247070312500000000000,\n",
       "            4.804588269442319869995117187500,  0.544819116592407226562500000000],\n",
       "          [-0.569639652967453002929687500000, -3.406021848320960998535156250000,\n",
       "            1.840629030019044876098632812500,  1.492054417729377746582031250000],\n",
       "          [ 0.100404828786849975585937500000, -1.853039517998695373535156250000,\n",
       "           -0.136249911040067672729492187500,  0.911580935120582580566406250000]]]],\n",
       "       dtype=torch.float64, grad_fn=<SlowConv2DBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32(input_fp32.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.,  0.,  1.],\n",
       "         [-1.,  0.,  1.],\n",
       "         [-1.,  0.,  1.]]]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter = np.zeros((3, 3, 1, 1))\n",
    "# filter[:, :, 1, 1] = model_fp32.conv.weight.detach().numpy()[0][0]\n",
    "np.reshape(model_fp32.conv.weight.detach().numpy()[0][0], (1, 1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 451, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(input_fp32[0][0].detach().numpy(), (4, 4, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32.conv.weight.detach().numpy()[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = convolve(np.reshape(input_fp32[0][0].detach().numpy(), (4, 4, 1)), np.reshape(model_fp32.conv.weight.detach().numpy()[0][0], (3, 3, 1, 1)), np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.03217167],\n",
       "        [-1.75511044],\n",
       "        [ 4.91004676],\n",
       "        [-0.17206544]],\n",
       "\n",
       "       [[-1.86825208],\n",
       "        [-2.3377552 ],\n",
       "        [ 4.80458827],\n",
       "        [ 0.54481912]],\n",
       "\n",
       "       [[-0.56963965],\n",
       "        [-3.40602185],\n",
       "        [ 1.84062903],\n",
       "        [ 1.49205442]],\n",
       "\n",
       "       [[ 0.10040483],\n",
       "        [-1.85303952],\n",
       "        [-0.13624991],\n",
       "        [ 0.91158094]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.03217167, -1.75511044,  4.91004676, -0.17206544],\n",
       "       [-1.86825208, -2.3377552 ,  4.80458827,  0.54481912],\n",
       "       [-0.56963965, -3.40602185,  1.84062903,  1.49205442],\n",
       "       [ 0.10040483, -1.85303952, -0.13624991,  0.91158094]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve(input_fp32[0][0].detach().numpy(), model_fp32.conv.weight.detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-2.03217167, -1.75511044,  4.91004676, -0.17206544],\n",
       "         [-1.86825208, -2.3377552 ,  4.80458827,  0.54481912],\n",
       "         [-0.56963965, -3.40602185,  1.84062903,  1.49205442],\n",
       "         [ 0.10040483, -1.85303952, -0.13624991,  0.91158094]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32(input_fp32.double()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(model_fp32(input_fp32.double()).detach().numpy() - convolve(input_fp32[0][0].detach().numpy(), model_fp32.conv.weight.detach().numpy()[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = input_fp32[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95466703, -1.3621272 ,  0.7525389 ,  1.5710406 ],\n",
       "       [ 0.97250885, -0.6700445 , -0.5804735 ,  1.3068345 ],\n",
       "       [-0.1342398 ,  0.16391958, -0.71688455,  0.0584611 ],\n",
       "       [ 1.0756984 , -0.06351475, -0.19469638, -0.09430618]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_(img, conv_filter):\n",
    "    filter_size = conv_filter.shape[1]\n",
    "    result = np.zeros((img.shape))\n",
    "    #Looping through the image to apply the convolution operation.\n",
    "    for r in np.uint16(np.arange(filter_size/2.0, \n",
    "                          img.shape[0]-filter_size/2.0+1)):\n",
    "        for c in np.uint16(np.arange(filter_size/2.0, \n",
    "                                           img.shape[1]-filter_size/2.0+1)):\n",
    "            \"\"\"\n",
    "            Getting the current region to get multiplied with the filter.\n",
    "            How to loop through the image and get the region based on \n",
    "            the image and filer sizes is the most tricky part of convolution.\n",
    "            \"\"\"\n",
    "            curr_region = img[r-np.uint16(np.floor(filter_size/2.0)):r+np.uint16(np.ceil(filter_size/2.0)), \n",
    "                              c-np.uint16(np.floor(filter_size/2.0)):c+np.uint16(np.ceil(filter_size/2.0))]\n",
    "            #Element-wise multipliplication between the current region and the filter.\n",
    "            curr_result = curr_region * conv_filter\n",
    "            conv_sum = np.sum(curr_result) #Summing the result of multiplication.\n",
    "            result[r, c] = conv_sum #Saving the summation in the convolution layer feature map.\n",
    "            \n",
    "    #Clipping the outliers of the result matrix.\n",
    "    final_result = result[np.uint16(filter_size/2.0):result.shape[0]-np.uint16(filter_size/2.0), \n",
    "                          np.uint16(filter_size/2.0):result.shape[1]-np.uint16(filter_size/2.0)]\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(img, conv_filter):\n",
    "    if len(img.shape) > 2 or len(conv_filter.shape) > 3: # Check if number of image channels matches the filter depth.\n",
    "        if img.shape[-1] != conv_filter.shape[-1]:\n",
    "            print(\"Error: Number of channels in both image and filter must match.\")\n",
    "            sys.exit()\n",
    "    if conv_filter.shape[1] != conv_filter.shape[2]: # Check if filter dimensions are equal.\n",
    "        print('Error: Filter must be a square matrix. I.e. number of rows and columns must match.')\n",
    "        sys.exit()\n",
    "    if conv_filter.shape[1]%2==0: # Check if filter diemnsions are odd.\n",
    "        print('Error: Filter must have an odd size. I.e. number of rows and columns must be odd.')\n",
    "        sys.exit()\n",
    "\n",
    "    # An empty feature map to hold the output of convolving the filter(s) with the image.\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filter.shape[1]+1, \n",
    "                                img.shape[1]-conv_filter.shape[1]+1, \n",
    "                                conv_filter.shape[0]))\n",
    "\n",
    "    # Convolving the image by the filter(s).\n",
    "    for filter_num in range(conv_filter.shape[0]):\n",
    "        print(\"Filter \", filter_num + 1)\n",
    "        curr_filter = conv_filter[filter_num, :] # getting a filter from the bank.\n",
    "        \"\"\" \n",
    "        Checking if there are mutliple channels for the single filter.\n",
    "        If so, then each channel will convolve the image.\n",
    "        The result of all convolutions are summed to return a single feature map.\n",
    "        \"\"\"\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # Array holding the sum of all feature maps.\n",
    "            for ch_num in range(1, curr_filter.shape[-1]): # Convolving each channel with the image and summing the results.\n",
    "                conv_map = conv_map + conv_(img[:, :, ch_num], \n",
    "                                  curr_filter[:, :, ch_num])\n",
    "        else: # There is just a single channel in the filter.\n",
    "            conv_map = conv_(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map # Holding feature map with the current filter.\n",
    "    return feature_maps # Returning all feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "# Reading the image\n",
    "#img = skimage.io.imread(\"fruits2.png\")\n",
    "img = skimage.data.chelsea()\n",
    "# Converting the image into gray.\n",
    "img = skimage.color.rgb2gray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Working with conv layer 1**\n",
      "Filter  1\n",
      "\n",
      "**ReLU**\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First conv layer\n",
    "#l1_filter = numpy.random.rand(2,7,7)*20 # Preparing the filters randomly.\n",
    "l1_filter = np.zeros((1,3,3))\n",
    "l1_filter[0, :, :] = np.array([[[-1, 0, 1], \n",
    "                                   [-1, 0, 1], \n",
    "                                   [-1, 0, 1]]])\n",
    "# l1_filter[1, :, :] = np.array([[[1,   1,  1], \n",
    "#                                    [0,   0,  0], \n",
    "#                                    [-1, -1, -1]]])\n",
    "print(\"\\n**Working with conv layer 1**\")\n",
    "l1_feature_map = conv(img, l1_filter)\n",
    "print(\"\\n**ReLU**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32[0].detach().numpy().shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter  1\n"
     ]
    }
   ],
   "source": [
    "l1_feature_map = conv(img, l1_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 449, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.tensor(img).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.485230980392156840963480135542, 0.485230980392156840963480135542,\n",
       "           0.477387843137254908043587420252,  ...,\n",
       "           0.116923921568627436906595562505, 0.116923921568627436906595562505,\n",
       "           0.116923921568627436906595562505],\n",
       "          [0.496995686274509795854470439735, 0.493074117647058829394524082090,\n",
       "           0.485230980392156840963480135542,  ...,\n",
       "           0.123368235294117639666566788037, 0.122534901960784300967155502349,\n",
       "           0.127289803921568633882088761311],\n",
       "          [0.508492549019607831262135277939, 0.504570980392156864802188920294,\n",
       "           0.494755686274509776101382385605,  ...,\n",
       "           0.123360392156862744283962740610, 0.127281960784313724621696906070,\n",
       "           0.131203529411764691081643263715],\n",
       "          ...,\n",
       "          [0.247867450980392145520880831100, 0.298847843137254876033637174260,\n",
       "           0.405012941176470575577184263238,  ...,\n",
       "           0.589148235294117639249122930778, 0.589148235294117639249122930778,\n",
       "           0.589148235294117639249122930778],\n",
       "          [0.381736470588235343726069004333, 0.424873725490196141318932632203,\n",
       "           0.396283921568627517384442171533,  ...,\n",
       "           0.574035294117647088008027367323, 0.574035294117647088008027367323,\n",
       "           0.577956862745098054467973724968],\n",
       "          [0.424873725490196141318932632203, 0.368832941176470585631363974244,\n",
       "           0.360424313725490219617597631441,  ...,\n",
       "           0.554427450980392255708295579097, 0.554427450980392255708295579097,\n",
       "           0.558349019607843111145939474227]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.resize_(1, 1, img.shape[0], img.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 300, 451])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 9.783050980392156148468529863749e-01,\n",
       "           -1.960784313725483229973178822547e-02,\n",
       "           -1.960784313725483229973178822547e-02,\n",
       "            ...,\n",
       "           -1.060313725490198699663579873231e-02,\n",
       "            3.921568627450994215521973274008e-03,\n",
       "           -2.394588235294117239959632570390e-01],\n",
       "          [ 1.482876078431372590671344369184e+00,\n",
       "           -3.334470588235288746048468055960e-02,\n",
       "           -3.334470588235288746048468055960e-02,\n",
       "            ...,\n",
       "           -8.921568627450998656414071774634e-03,\n",
       "            1.176470588235294101320249637865e-02,\n",
       "           -3.667407843137254763732357787376e-01],\n",
       "          [ 1.510341960784313819488033914240e+00,\n",
       "           -4.118784313725498691383108962327e-02,\n",
       "           -3.334470588235305399393837433308e-02,\n",
       "            ...,\n",
       "            3.103137254901966457509843166918e-03,\n",
       "            1.625176470588236832259809716561e-02,\n",
       "           -3.874796078431372303718660532468e-01],\n",
       "          ...,\n",
       "          [ 9.256839215686274435768154944526e-01,\n",
       "            2.114741176470588679414674970758e-01,\n",
       "            1.447847058823528432203175952964e-01,\n",
       "            ...,\n",
       "            5.030588235294342069892081781290e-03,\n",
       "           -1.110223024625156540423631668091e-16,\n",
       "           -1.768889019607843060555296688108e+00],\n",
       "          [ 1.092554509803921547472782549448e+00,\n",
       "            1.072435294117646820133415985765e-01,\n",
       "           -1.656509803921579759844462387264e-02,\n",
       "            ...,\n",
       "            8.401568627451116988424928422319e-03,\n",
       "            7.843137254901821897590252774535e-03,\n",
       "           -1.717610980392156871943143414683e+00],\n",
       "          [ 7.937066666666667824614478377043e-01,\n",
       "           -4.990196078431374804296183356200e-02,\n",
       "           -1.050717647058824755212924628722e-01,\n",
       "            ...,\n",
       "            7.300392156862844927900368929841e-03,\n",
       "            7.843137254901821897590252774535e-03,\n",
       "           -1.128462745098039343716322946420e+00]]]], dtype=torch.float64,\n",
       "       grad_fn=<SlowConv2DBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.33447059e-02],\n",
       "        [-3.33447059e-02],\n",
       "        [-5.60313725e-03],\n",
       "        ...,\n",
       "        [-1.87141176e-02],\n",
       "        [-8.92156863e-03],\n",
       "        [ 1.17647059e-02]],\n",
       "\n",
       "       [[-4.11878431e-02],\n",
       "        [-3.33447059e-02],\n",
       "        [-9.80745098e-03],\n",
       "        ...,\n",
       "        [-1.45325490e-02],\n",
       "        [ 3.10313725e-03],\n",
       "        [ 1.62517647e-02]],\n",
       "\n",
       "       [[-4.17533333e-02],\n",
       "        [-2.99886275e-02],\n",
       "        [-1.40117647e-02],\n",
       "        ...,\n",
       "        [-1.42725490e-02],\n",
       "        [ 5.33529412e-03],\n",
       "        [ 1.62517647e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.02529804e-01],\n",
       "        [ 2.32763529e-01],\n",
       "        [ 1.57650588e-01],\n",
       "        ...,\n",
       "        [-5.86313725e-03],\n",
       "        [ 1.98000000e-03],\n",
       "        [-3.92156863e-03]],\n",
       "\n",
       "       [[ 2.11474118e-01],\n",
       "        [ 1.44784706e-01],\n",
       "        [ 1.73301961e-02],\n",
       "        ...,\n",
       "        [ 1.10901961e-03],\n",
       "        [ 5.03058824e-03],\n",
       "        [-1.11022302e-16]],\n",
       "\n",
       "       [[ 1.07243529e-01],\n",
       "        [-1.65650980e-02],\n",
       "        [-1.73390196e-01],\n",
       "        ...,\n",
       "        [ 8.40156863e-03],\n",
       "        [ 8.40156863e-03],\n",
       "        [ 7.84313725e-03]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1048f3b05e85321c8253e095fcb57a50212287a1649d557864a0f57b3496baef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import torch.quantization\n",
    "torch.set_printoptions(precision=30)\n",
    "np.set_printoptions(precision=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_quant_fullweight(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "            super(M_quant_fullweight, self).__init__()\n",
    "            # QuantStub converts tensors from floating point to quantized\n",
    "            self.quant = torch.quantization.QuantStub()\n",
    "            # self.BN = nn.BatchNorm2d(3)\n",
    "            # self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "            # self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "            # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.03307433053851127625, -0.13484150171279907227, -0.21625524759292602539], \n",
    "            #                                                         [ 0.14247404038906097412, -0.14247404038906097412, -0.24932956695556640625], \n",
    "            #                                                         [ 0.32311078906059265137, -0.14501821994781494141, -0.21371106803417205811]]]]))\n",
    "            self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.0, 0.0, 0.0], \n",
    "                                                                    [ 0.0, 0.5, 0.0], \n",
    "                                                                    [ 0.0, 0.0, 0.0]]]]))\n",
    "            self.conv.bias = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "            # self.relu = torch.nn.ReLU()\n",
    "            # DeQuantStub converts tensors from quantized to floating point\n",
    "            self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.quant(x)\n",
    "        # x = self.BN(x)\n",
    "        x = self.conv(x)\n",
    "        # x = self.relu(x)      \n",
    "        x = self.dequant(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(input_image: np.array, kernel: np.array, bias: np.array) -> np.array:    \n",
    "    ### Add description of what dimension is what\n",
    "    k = kernel.shape[0]\n",
    "    number_input_row = input_image.shape[0]\n",
    "    number_input_col = input_image.shape[1]\n",
    "    input_depth = input_image.shape[2]   # R, G, B channel\n",
    "    filter_depth = kernel.shape[2]       # filter depth\n",
    "    filter_num = kernel.shape[3]         # number of filters\n",
    "    bias_depth = bias.shape[0]\n",
    "\n",
    "    if input_depth != filter_depth:\n",
    "        print(\"Error: Number of channels in both image and filter depth must match.\")\n",
    "        sys.exit()\n",
    "\n",
    "    if bias_depth != filter_num:\n",
    "        print(\"Error: Bias depth and filter number must match.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    if np.isnan(input_image.all()):\n",
    "        print(\"Input image has NaN values\")\n",
    "        sys.exit()\n",
    "\n",
    "    if np.isnan(kernel.all()):\n",
    "        print(\"Kernel has NaN values\")\n",
    "        sys.exit()\n",
    "\n",
    "    padded_image = np.zeros(shape=(number_input_row+int((k+1)/2), number_input_col+int((k+1)/2), input_depth))\n",
    "    padded_image[((k-1)>>1):number_input_row + ((k-1)>>1), ((k-1)>>1):number_input_col + ((k-1)>>1), :] = input_image\n",
    "    convolved_img = np.zeros(shape=(number_input_row, number_input_col, filter_num))\n",
    "\n",
    "    for f in range(filter_num):\n",
    "        for i in range(number_input_row):\n",
    "            for j in range(number_input_col):\n",
    "                mat = padded_image[i:i+k, j:j+k, :]\n",
    "                convolved_img[i, j, f ] = np.sum(np.multiply(mat, kernel[:,:,:,f]))\n",
    "        \n",
    "        convolved_img[:, :, f ] += bias[f]\n",
    "\n",
    "\n",
    "    if np.isnan(convolved_img.all()):\n",
    "        print(\"convolved Image has NaN values\")\n",
    "        sys.exit()\n",
    "\n",
    "    return convolved_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp32 = torch.tensor([[[[ 0.95466703176498413086, -0.136212718486785888672,\n",
    "            0.75253891944885253906,  1.57104063034057617188],\n",
    "          [ 0.97250884771347045898, -0.67004448175430297852,\n",
    "           -0.58047348260879516602,  1.30683445930480957031],\n",
    "          [-0.13423979282379150391,  0.16391958296298980713,\n",
    "           -0.71688455343246459961,  0.05846109613776206970],\n",
    "          [ 1.07569837570190429688, -0.06351475417613983154,\n",
    "           -0.19469638168811798096, -0.09430617839097976685]]]], requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp32_debug = torch.tensor([[[[ 0.95466703176498413086, 0.0, 0.0,  0.0],\n",
    "          [ 0.0, 0.0, 0.0,  0.0],\n",
    "          [ 0.0, 0.0, 0.0,  0.0],\n",
    "          [ 0.0, 0.0, 0.0,  0.0]]]], requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:172: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
     ]
    }
   ],
   "source": [
    "model_quant = M_quant_fullweight()\n",
    "model_quant.eval()\n",
    "model_quant.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, [['conv', 'relu']])\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, ['conv'])\n",
    "model_quant_fullweight = torch.quantization.prepare(model_quant)\n",
    "model_quant_fullweight(input_fp32)   ### Passing the input through model before conversion for calibration\n",
    "model_fp32_converted_fullweight = torch.quantization.convert(model_quant_fullweight, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_quant = model_fp32_converted_fullweight(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.477168589830398559570312500000, -0.072025448083877563476562500000,\n",
       "            0.369130432605743408203125000000,  0.783276736736297607421875000000],\n",
       "          [ 0.486171782016754150390625000000, -0.288101792335510253906250000000,\n",
       "           -0.288101792335510253906250000000,  0.648229002952575683593750000000],\n",
       "          [-0.072025448083877563476562500000,  0.081028625369071960449218750000,\n",
       "           -0.288101792335510253906250000000,  0.027009543031454086303710937500],\n",
       "          [ 0.531187653541564941406250000000, -0.036012724041938781738281250000,\n",
       "           -0.090031810104846954345703125000, -0.054019086062908172607421875000]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quantize(scale=tensor([0.016968380659818649291992187500]), zero_point=tensor([34]), dtype=torch.quint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), scale=0.009003181010484695, zero_point=40, padding=(1, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[[[0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.498039245605468750000000000000,\n",
    "           0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.conv._weight_bias()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 90,  26,  78, 127],\n",
       "          [ 91,   0,   0, 111],\n",
       "          [ 26,  44,   0,  37],\n",
       "          [ 97,  30,  23,  28]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quant Block Output\n",
    "model_fp32_converted_fullweight.quant(input_fp32).int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.477168589830398559570312500000, -0.072025448083877563476562500000,\n",
       "            0.369130432605743408203125000000,  0.783276736736297607421875000000],\n",
       "          [ 0.486171782016754150390625000000, -0.288101792335510253906250000000,\n",
       "           -0.288101792335510253906250000000,  0.648229002952575683593750000000],\n",
       "          [-0.072025448083877563476562500000,  0.081028625369071960449218750000,\n",
       "           -0.288101792335510253906250000000,  0.027009543031454086303710937500],\n",
       "          [ 0.531187653541564941406250000000, -0.036012724041938781738281250000,\n",
       "           -0.090031810104846954345703125000, -0.054019086062908172607421875000]]]],\n",
       "       size=(1, 1, 4, 4), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.009003181010484695,\n",
       "       zero_point=40)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.conv(model_fp32_converted_fullweight.quant(input_fp32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 93,  32,  81, 127],\n",
       "          [ 94,   8,   8, 112],\n",
       "          [ 32,  49,   8,  43],\n",
       "          [ 99,  36,  30,  34]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv Block Output\n",
    "model_fp32_converted_fullweight.conv(model_fp32_converted_fullweight.quant(input_fp32)).int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[90, 34, 34, 34],\n",
       "          [34, 34, 34, 34],\n",
       "          [34, 34, 34, 34],\n",
       "          [34, 34, 34, 34]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.quant(input_fp32_debug).int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.950229287147521972656250000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000]]]],\n",
       "       size=(1, 1, 4, 4), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.01696838065981865,\n",
       "       zero_point=34)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.quant(input_fp32_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.477168589830398559570312500000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000]]]],\n",
       "       size=(1, 1, 4, 4), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.009003181010484695,\n",
       "       zero_point=40)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.conv(model_fp32_converted_fullweight.quant(input_fp32_debug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[93, 40, 40, 40],\n",
       "          [40, 40, 40, 40],\n",
       "          [40, 40, 40, 40],\n",
       "          [40, 40, 40, 40]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv Block Output\n",
    "model_fp32_converted_fullweight.conv(model_fp32_converted_fullweight.quant(input_fp32_debug)).int_repr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Weight Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0,   0,   0],\n",
       "          [  0, 127,   0],\n",
       "          [  0,   0,   0]]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.conv._weight_bias()[0].int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.498039245605468750000000000000,\n",
       "           0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000]]]], size=(1, 1, 3, 3),\n",
       "       dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
       "       scale=tensor([0.003921568859368562698364257812], dtype=torch.float64),\n",
       "       zero_point=tensor([0]), axis=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantized weights from pytroch module\n",
    "model_fp32_converted_fullweight.conv._weight_bias()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convolution weight block zero point\n",
    "model_fp32_converted_fullweight.conv._weight_bias()[0].q_per_channel_zero_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.003921568859368562698364257812], dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convolution weight block scale\n",
    "model_fp32_converted_fullweight.conv._weight_bias()[0].q_per_channel_scales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.500000000000000000000000000000,\n",
       "           0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Weight before quantization\n",
    "model_quant.conv.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.498039245605468750000000000000,\n",
       "           0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000]]]], size=(1, 1, 3, 3),\n",
       "       dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine,\n",
       "       scale=0.003921568859368563, zero_point=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually quantizing weight\n",
    "torch.quantize_per_tensor(model_quant.conv.weight.detach(), scale = model_fp32_converted_fullweight.conv._weight_bias()[0].q_per_channel_scales(), zero_point = model_fp32_converted_fullweight.conv._weight_bias()[0].q_per_channel_zero_points(), dtype=torch.quint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_conv_weight = (torch.round(model_quant.conv.weight.detach()/model_fp32_converted_fullweight.conv._weight_bias()[0].q_per_channel_scales()) + model_fp32_converted_fullweight.conv._weight_bias()[0].q_per_channel_zero_points()).type(dtype=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0.,   0.,   0.],\n",
       "          [  0., 127.,   0.],\n",
       "          [  0.,   0.,   0.]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_conv_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_conv_weight = (quant_conv_weight - model_fp32_converted_fullweight.conv._weight_bias()[0].q_per_channel_zero_points())*model_fp32_converted_fullweight.conv._weight_bias()[0].q_per_channel_scales().type(dtype=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.001953095194483694735029799538,\n",
       "           0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_conv_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Weights matched verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Block Output verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_conv_weight = torch.tensor([[[[0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.49803924560546875000,\n",
    "           0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_quant_manual_after_quant = torch.clamp(torch.round(input_fp32.detach()/model_fp32_converted_fullweight.quant.scale)+model_fp32_converted_fullweight.quant.zero_point, min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 90.,  26.,  78., 127.],\n",
       "          [ 91.,   0.,   0., 111.],\n",
       "          [ 26.,  44.,   0.,  37.],\n",
       "          [ 97.,  30.,  23.,  28.]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_quant_manual_after_quant\n",
    "#matching with pytorch quant block output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_quant_manual_after_quant_dequant = (input_quant_manual_after_quant - model_fp32_converted_fullweight.quant.zero_point)*model_fp32_converted_fullweight.quant.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.950229287147521972656250000000, -0.135747045278549194335937500000,\n",
       "            0.746608734130859375000000000000,  1.578059434890747070312500000000],\n",
       "          [ 0.967197716236114501953125000000, -0.576924920082092285156250000000,\n",
       "           -0.576924920082092285156250000000,  1.306565284729003906250000000000],\n",
       "          [-0.135747045278549194335937500000,  0.169683814048767089843750000000,\n",
       "           -0.576924920082092285156250000000,  0.050905141979455947875976562500],\n",
       "          [ 1.069007992744445800781250000000, -0.067873522639274597167968750000,\n",
       "           -0.186652183532714843750000000000, -0.101810283958911895751953125000]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_quant_manual_after_quant_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolve_manual_float = convolve(np.reshape(input_quant_manual_after_quant_dequant.detach().numpy()[0][0].astype(np.float32), (4, 4, 1)), np.reshape(quant_conv_weight.detach().numpy()[0][0].astype(np.float32), (3, 3, 1, 1)), np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.4732514773231742  ],\n",
       "        [-0.06760735602370005 ],\n",
       "        [ 0.3718404507089872  ],\n",
       "        [ 0.78593553047358    ]],\n",
       "\n",
       "       [[ 0.4817024209455667  ],\n",
       "        [-0.2873312519686806  ],\n",
       "        [-0.2873312519686806  ],\n",
       "        [ 0.6507207887407276  ]],\n",
       "\n",
       "       [[-0.06760735602370005 ],\n",
       "        [ 0.0845091987403066  ],\n",
       "        [-0.2873312519686806  ],\n",
       "        [ 0.02535275850888752 ]],\n",
       "\n",
       "       [[ 0.5324079342526602  ],\n",
       "        [-0.033803678011850025],\n",
       "        [-0.0929601126772468  ],\n",
       "        [-0.05070551701777504 ]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve_manual_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manual_integer = np.round(convolve_manual_float/model_fp32_converted_fullweight.conv.scale) + model_fp32_converted_fullweight.conv.zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 93.],\n",
       "        [ 32.],\n",
       "        [ 81.],\n",
       "        [127.]],\n",
       "\n",
       "       [[ 94.],\n",
       "        [  8.],\n",
       "        [  8.],\n",
       "        [112.]],\n",
       "\n",
       "       [[ 32.],\n",
       "        [ 49.],\n",
       "        [  8.],\n",
       "        [ 43.]],\n",
       "\n",
       "       [[ 99.],\n",
       "        [ 36.],\n",
       "        [ 30.],\n",
       "        [ 34.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_manual_integer\n",
    "# matching with that of pytorch block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manual = (torch.tensor(output_manual_integer, dtype=torch.float32) - model_fp32_converted_fullweight.conv.zero_point)*torch.tensor(model_fp32_converted_fullweight.conv.scale, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.47716859355568886 ],\n",
       "        [-0.07202544808387756 ],\n",
       "        [ 0.3691304214298725  ],\n",
       "        [ 0.7832767479121685  ]],\n",
       "\n",
       "       [[ 0.48617177456617355 ],\n",
       "        [-0.28810179233551025 ],\n",
       "        [-0.28810179233551025 ],\n",
       "        [ 0.6482290327548981  ]],\n",
       "\n",
       "       [[-0.07202544808387756 ],\n",
       "        [ 0.08102862909436226 ],\n",
       "        [-0.28810179233551025 ],\n",
       "        [ 0.027009543031454086]],\n",
       "\n",
       "       [[ 0.531187679618597   ],\n",
       "        [-0.03601272404193878 ],\n",
       "        [-0.09003181010484695 ],\n",
       "        [-0.05401908606290817 ]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(output_manual, (1, 4, 4)) - output_quant.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Results matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp32_debug_quant =  torch.tensor([[[[0.950229287147521972656250000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000, 0.000000000000000000000000000000]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolve_manual_debug = convolve(np.reshape(input_fp32_debug_quant.detach().numpy()[0][0].astype(np.float32), (4, 4, 1)), np.reshape(quant_conv_weight.detach().numpy()[0][0].astype(np.float32), (3, 3, 1, 1)), np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.4732514773231742],\n",
       "        [0.                ],\n",
       "        [0.                ],\n",
       "        [0.                ]],\n",
       "\n",
       "       [[0.                ],\n",
       "        [0.                ],\n",
       "        [0.                ],\n",
       "        [0.                ]],\n",
       "\n",
       "       [[0.                ],\n",
       "        [0.                ],\n",
       "        [0.                ],\n",
       "        [0.                ]],\n",
       "\n",
       "       [[0.                ],\n",
       "        [0.                ],\n",
       "        [0.                ],\n",
       "        [0.                ]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve_manual_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[93.],\n",
       "        [40.],\n",
       "        [40.],\n",
       "        [40.]],\n",
       "\n",
       "       [[40.],\n",
       "        [40.],\n",
       "        [40.],\n",
       "        [40.]],\n",
       "\n",
       "       [[40.],\n",
       "        [40.],\n",
       "        [40.],\n",
       "        [40.]],\n",
       "\n",
       "       [[40.],\n",
       "        [40.],\n",
       "        [40.],\n",
       "        [40.]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(convolve_manual_debug/model_fp32_converted_fullweight.conv.scale) + model_fp32_converted_fullweight.conv.zero_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.477168589830398559570312500000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000]]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight(input_fp32_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.477168589830398559570312500000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000],\n",
       "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
       "           0.000000000000000000000000000000, 0.000000000000000000000000000000]]]],\n",
       "       size=(1, 1, 4, 4), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.009003181010484695,\n",
       "       zero_point=40)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.conv(model_fp32_converted_fullweight.quant(input_fp32_debug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_fp32_converted_fullweight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a17443d892d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_fp32_converted_fullweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fp32_debug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_fp32_converted_fullweight' is not defined"
     ]
    }
   ],
   "source": [
    "model_fp32_converted_fullweight.quant(input_fp32_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.4732514773231742  ],\n",
       "        [-0.06760735602370005 ],\n",
       "        [ 0.3718404507089872  ],\n",
       "        [ 0.78593553047358    ]],\n",
       "\n",
       "       [[ 0.4817024209455667  ],\n",
       "        [-0.2873312519686806  ],\n",
       "        [-0.2873312519686806  ],\n",
       "        [ 0.6507207887407276  ]],\n",
       "\n",
       "       [[-0.06760735602370005 ],\n",
       "        [ 0.0845091987403066  ],\n",
       "        [-0.2873312519686806  ],\n",
       "        [ 0.02535275850888752 ]],\n",
       "\n",
       "       [[ 0.5324079342526602  ],\n",
       "        [-0.033803678011850025],\n",
       "        [-0.0929601126772468  ],\n",
       "        [-0.05070551701777504 ]]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve(np.reshape(input_quant_manual_after_quant_dequant.detach().numpy()[0][0].astype(np.float32), (4, 4, 1)), np.reshape(quant_conv_weight.detach().numpy()[0][0].astype(np.float32), (3, 3, 1, 1)), np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9502293  , -0.13574705 ,  0.74660873 ,  1.5780594  ],\n",
       "       [ 0.9671977  , -0.5769249  , -0.5769249  ,  1.3065653  ],\n",
       "       [-0.13574705 ,  0.16968381 , -0.5769249  ,  0.050905142],\n",
       "       [ 1.069008   , -0.06787352 , -0.18665218 , -0.101810284]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_quant_manual_after_quant_dequant.detach().numpy()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_conv_weight = torch.tensor([[[[0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.49803924560546875000,\n",
    "           0.000000000000000000000000000000],\n",
    "          [0.000000000000000000000000000000, 0.000000000000000000000000000000,\n",
    "           0.000000000000000000000000000000]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_converted_fullweight.conv._weight_bias()[0].int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolve_manual = convolve(np.reshape(input_quant_manual_after_quant.detach().numpy(), (4, 4, 1)), np.reshape(model_fp32_converted_fullweight.conv._weight_bias()[0].int_repr().detach().numpy()[0][0], (3, 3, 1, 1)), np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[11430.],\n",
       "        [ 3302.],\n",
       "        [ 9906.],\n",
       "        [16129.]],\n",
       "\n",
       "       [[11557.],\n",
       "        [    0.],\n",
       "        [    0.],\n",
       "        [14097.]],\n",
       "\n",
       "       [[ 3302.],\n",
       "        [ 5588.],\n",
       "        [    0.],\n",
       "        [ 4699.]],\n",
       "\n",
       "       [[12319.],\n",
       "        [ 3810.],\n",
       "        [ 2921.],\n",
       "        [ 3556.]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_quant_fullweight_bias(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "            super(M_quant_fullweight_bias, self).__init__()\n",
    "            # QuantStub converts tensors from floating point to quantized\n",
    "            self.quant = torch.quantization.QuantStub()\n",
    "            # self.BN = nn.BatchNorm2d(3)\n",
    "            # self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "            # self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.03307433053851127625, -0.13484150171279907227, -0.21625524759292602539], \n",
    "                                                                    [ 0.14247404038906097412, -0.14247404038906097412, -0.24932956695556640625], \n",
    "                                                                    [ 0.32311078906059265137, -0.14501821994781494141, -0.21371106803417205811]]]]))\n",
    "            # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.0, 0.0, 0.0], \n",
    "            #                                                         [ 0.0, 0.5, 0.0], \n",
    "            #                                                         [ 0.0, 0.0, 0.0]]]]))\n",
    "            # self.conv.bias = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "            self.conv.bias = torch.nn.Parameter(torch.tensor([2.12300]))\n",
    "            # self.conv.bias = torch.nn.Parameter(torch.tensor([2.11232137680053710936300]))\n",
    "            # 2.11232137680053710937500\n",
    "            # self.relu = torch.nn.ReLU()\n",
    "            # DeQuantStub converts tensors from quantized to floating point\n",
    "            self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.quant(x)\n",
    "        # x = self.BN(x)\n",
    "        x = self.conv(x)\n",
    "        # x = self.relu(x)      \n",
    "        x = self.dequant(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:172: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
      "/home/avishek/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
     ]
    }
   ],
   "source": [
    "model_quant_bias = M_quant_fullweight_bias()\n",
    "model_quant_bias.eval()\n",
    "model_quant_bias.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, [['conv', 'relu']])\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, ['conv'])\n",
    "model_quant_fullweight_bias = torch.quantization.prepare(model_quant_bias)\n",
    "model_quant_fullweight_bias(input_fp32)   ### Passing the input through model before conversion for calibration\n",
    "model_fp32_converted_fullweight_bias = torch.quantization.convert(model_quant_fullweight_bias, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Quantization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quantize(scale=tensor([0.016968380659818649291992187500]), zero_point=tensor([34]), dtype=torch.quint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), scale=0.02287949062883854, zero_point=0, padding=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.032944630831480026245117187500, -0.134312719106674194335937500000,\n",
       "            -0.215407192707061767578125000000],\n",
       "           [ 0.141915321350097656250000000000, -0.141915321350097656250000000000,\n",
       "            -0.248351827263832092285156250000],\n",
       "           [ 0.321843683719635009765625000000, -0.144449532032012939453125000000,\n",
       "            -0.212872996926307678222656250000]]]], size=(1, 1, 3, 3),\n",
       "        dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
       "        scale=tensor([0.002534202300012111663818359375], dtype=torch.float64),\n",
       "        zero_point=tensor([0]), axis=0),\n",
       " Parameter containing:\n",
       " tensor([2.122999906539916992187500000000], requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.conv._weight_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 13, -53, -85],\n",
       "          [ 56, -56, -98],\n",
       "          [127, -57, -84]]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.conv._weight_bias()[0].int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.950229287147521972656250000000, -0.135747045278549194335937500000,\n",
       "            0.746608734130859375000000000000,  1.578059434890747070312500000000],\n",
       "          [ 0.967197716236114501953125000000, -0.576924920082092285156250000000,\n",
       "           -0.576924920082092285156250000000,  1.306565284729003906250000000000],\n",
       "          [-0.135747045278549194335937500000,  0.169683814048767089843750000000,\n",
       "           -0.576924920082092285156250000000,  0.050905141979455947875976562500],\n",
       "          [ 1.069007992744445800781250000000, -0.067873522639274597167968750000,\n",
       "           -0.186652183532714843750000000000, -0.101810283958911895751953125000]]]],\n",
       "       size=(1, 1, 4, 4), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.01696838065981865,\n",
       "       zero_point=34)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.quant(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 90,  26,  78, 127],\n",
       "          [ 91,   0,   0, 111],\n",
       "          [ 26,  44,   0,  37],\n",
       "          [ 97,  30,  23,  28]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.quant(input_fp32).int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.013395071029663085937500000000, 2.608261823654174804687500000000,\n",
       "           1.235492467880249023437500000000, 1.624443888664245605468750000000],\n",
       "          [2.013395071029663085937500000000, 2.425225973129272460937500000000,\n",
       "           1.487166881561279296875000000000, 1.464287400245666503906250000000],\n",
       "          [1.944756746292114257812500000000, 2.859936237335205078125000000000,\n",
       "           2.013395071029663085937500000000, 1.784600257873535156250000000000],\n",
       "          [1.967636227607727050781250000000, 2.425225973129272460937500000000,\n",
       "           2.242190122604370117187500000000, 2.082033634185791015625000000000]]]],\n",
       "       size=(1, 1, 4, 4), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.02287949062883854,\n",
       "       zero_point=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.conv(model_fp32_converted_fullweight_bias.quant(input_fp32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 88, 114,  54,  71],\n",
       "          [ 88, 106,  65,  64],\n",
       "          [ 85, 125,  88,  78],\n",
       "          [ 86, 106,  98,  91]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.conv(model_fp32_converted_fullweight_bias.quant(input_fp32)).int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([2.122999906539916992187500000000], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant_bias.conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quant_conv_bias = (torch.round(model_quant_bias.conv.bias/(model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_scales()*model_fp32_converted_fullweight_bias.conv.scale))- model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_zero_points()).type(dtype=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23639.], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quant_conv_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.003921568859368562698364257812])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_scales().type(dtype=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.112329006195068359375000000000], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ((quant_conv_bias - model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_zero_points())*model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_scales().type(dtype=torch.FloatTensor)*model_fp32_converted_fullweight_bias.conv.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([2.122999906539916992187500000000], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias.conv._weight_bias()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Weight Verification same as the non bias case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_conv_weight = (torch.round(model_quant_bias.conv.weight.detach()/model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_scales()) + model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_zero_points()).type(dtype=torch.FloatTensor)\n",
    "quant_conv_weight = (quant_conv_weight - model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_zero_points())*model_fp32_converted_fullweight_bias.conv._weight_bias()[0].q_per_channel_scales().type(dtype=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_quant_manual_after_quant = torch.clamp(torch.round(input_fp32.detach()/model_fp32_converted_fullweight_bias.quant.scale)+model_fp32_converted_fullweight_bias.quant.zero_point, min=0)\n",
    "input_quant_manual_after_quant_dequant = (input_quant_manual_after_quant - model_fp32_converted_fullweight_bias.quant.zero_point)*model_fp32_converted_fullweight_bias.quant.scale\n",
    "# convolve_manual_float = convolve(np.reshape(input_quant_manual_after_quant_dequant.detach().numpy()[0][0].astype(np.float32), (input_fp32.shape[2], input_fp32.shape[3], input_fp32.shape[1])), np.reshape(quant_conv_weight.detach().numpy().astype(np.float32), (quant_conv_weight.shape[2], quant_conv_weight.shape[3], quant_conv_weight.shape[0], quant_conv_weight.shape[1])), np.array([0]))\n",
    "convolve_manual_float = convolve(np.reshape(input_quant_manual_after_quant_dequant.detach().numpy()[0][0].astype(np.float32), (input_fp32.shape[2], input_fp32.shape[3], input_fp32.shape[1])), np.reshape(quant_conv_weight.detach().numpy().astype(np.float32), (quant_conv_weight.shape[2], quant_conv_weight.shape[3], quant_conv_weight.shape[0], quant_conv_weight.shape[1])), model_fp32_converted_fullweight_bias.conv._weight_bias()[1].detach().numpy())\n",
    "output_manual_integer = np.round(convolve_manual_float/model_fp32_converted_fullweight_bias.conv.scale) + model_fp32_converted_fullweight_bias.conv.zero_point\n",
    "output_manual = (torch.tensor(output_manual_integer, dtype=torch.float32) - model_fp32_converted_fullweight_bias.conv.zero_point)*torch.tensor(model_fp32_converted_fullweight_bias.conv.scale, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 90.,  26.,  78., 127.],\n",
       "          [ 91.,   0.,   0., 111.],\n",
       "          [ 26.,  44.,   0.,  37.],\n",
       "          [ 97.,  30.,  23.,  28.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_quant_manual_after_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.950229287147521972656250000000, -0.135747045278549194335937500000,\n",
       "            0.746608734130859375000000000000,  1.578059434890747070312500000000],\n",
       "          [ 0.967197716236114501953125000000, -0.576924920082092285156250000000,\n",
       "           -0.576924920082092285156250000000,  1.306565284729003906250000000000],\n",
       "          [-0.135747045278549194335937500000,  0.169683814048767089843750000000,\n",
       "           -0.576924920082092285156250000000,  0.050905141979455947875976562500],\n",
       "          [ 1.069007992744445800781250000000, -0.067873522639274597167968750000,\n",
       "           -0.186652183532714843750000000000, -0.101810283958911895751953125000]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_quant_manual_after_quant_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.004961317885303 ],\n",
       "        [2.6091296907036874],\n",
       "        [1.225390583748318 ],\n",
       "        [1.6305919276975587]],\n",
       "\n",
       "       [[2.014120595676534 ],\n",
       "        [2.4287392012361857],\n",
       "        [1.4809473607325894],\n",
       "        [1.4753142189279387]],\n",
       "\n",
       "       [[1.9545207649587837],\n",
       "        [2.850152028546039 ],\n",
       "        [2.020140772369282 ],\n",
       "        [1.7940399084887053]],\n",
       "\n",
       "       [[1.9698292478240498],\n",
       "        [2.4277071696669   ],\n",
       "        [2.237254380045981 ],\n",
       "        [2.0851157545525902]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve_manual_float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 88.],\n",
       "        [114.],\n",
       "        [ 54.],\n",
       "        [ 71.]],\n",
       "\n",
       "       [[ 88.],\n",
       "        [106.],\n",
       "        [ 65.],\n",
       "        [ 64.]],\n",
       "\n",
       "       [[ 85.],\n",
       "        [125.],\n",
       "        [ 88.],\n",
       "        [ 78.]],\n",
       "\n",
       "       [[ 86.],\n",
       "        [106.],\n",
       "        [ 98.],\n",
       "        [ 91.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_manual_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.013395071029663085937500000000, 2.608261823654174804687500000000,\n",
       "          1.235492467880249023437500000000, 1.624443888664245605468750000000],\n",
       "         [2.013395071029663085937500000000, 2.425225973129272460937500000000,\n",
       "          1.487166881561279296875000000000, 1.464287400245666503906250000000],\n",
       "         [1.944756746292114257812500000000, 2.859936237335205078125000000000,\n",
       "          2.013395071029663085937500000000, 1.784600257873535156250000000000],\n",
       "         [1.967636227607727050781250000000, 2.425225973129272460937500000000,\n",
       "          2.242190122604370117187500000000, 2.082033634185791015625000000000]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_manual.reshape(1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.013395071029663085937500000000, 2.608261823654174804687500000000,\n",
       "          1.235492467880249023437500000000, 1.624443888664245605468750000000],\n",
       "         [2.013395071029663085937500000000, 2.425225973129272460937500000000,\n",
       "          1.487166881561279296875000000000, 1.464287400245666503906250000000],\n",
       "         [1.944756746292114257812500000000, 2.859936237335205078125000000000,\n",
       "          2.013395071029663085937500000000, 1.784600257873535156250000000000],\n",
       "         [1.967636227607727050781250000000, 2.425225973129272460937500000000,\n",
       "          2.242190122604370117187500000000, 2.082033634185791015625000000000]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_converted_fullweight_bias((input_fp32))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_manual.reshape(1, 4, 4) - model_fp32_converted_fullweight_bias((input_fp32))[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Results are matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_quant_fullweight_bias2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "            super(M_quant_fullweight_bias2, self).__init__()\n",
    "            # QuantStub converts tensors from floating point to quantized\n",
    "            self.quant = torch.quantization.QuantStub()\n",
    "            # self.BN = nn.BatchNorm2d(3)\n",
    "            # self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "            # self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv = torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "            # self.conv.bias = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "            # self.conv.bias = torch.nn.Parameter(torch.tensor([2.12300]))\n",
    "            # self.relu = torch.nn.ReLU()\n",
    "            # DeQuantStub converts tensors from quantized to floating point\n",
    "            self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = self.quant(x)\n",
    "        # x = self.BN(x)\n",
    "        x = self.conv(x)\n",
    "        # x = self.relu(x)      \n",
    "        # x = self.dequant(x)\n",
    "\n",
    "        return x\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M_quant_fullweight_bias2(\n",
       "  (quant): QuantStub()\n",
       "  (conv): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant_bias2 = M_quant_fullweight_bias2()\n",
    "model_quant_bias2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.172700405120849609375000000000,  0.035670280456542968750000000000,\n",
       "           -0.009075443260371685028076171875],\n",
       "          [ 0.219669222831726074218750000000, -0.273351132869720458984375000000,\n",
       "            0.183603286743164062500000000000],\n",
       "          [-0.041898608207702636718750000000,  0.143252611160278320312500000000,\n",
       "            0.286942601203918457031250000000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.043319266289472579956054687500, -0.117927871644496917724609375000,\n",
       "           -0.044052362442016601562500000000],\n",
       "          [-0.305706858634948730468750000000, -0.012369632720947265625000000000,\n",
       "           -0.066270038485527038574218750000],\n",
       "          [ 0.177178949117660522460937500000,  0.223731324076652526855468750000,\n",
       "           -0.018801212310791015625000000000]]]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant_bias2.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11298935,  0.32182777], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant_bias2.conv.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp32 = torch.rand(1, 1, 9, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fp32 = model_quant_bias2(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 9, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fp32.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant_bias2.conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.1727004  ,  0.03567028 , -0.009075443],\n",
       "         [ 0.21966922 , -0.27335113 ,  0.18360329 ],\n",
       "         [-0.04189861 ,  0.14325261 ,  0.2869426  ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.043319266, -0.11792787 , -0.044052362],\n",
       "         [-0.30570686 , -0.012369633, -0.06627004 ],\n",
       "         [ 0.17717895 ,  0.22373132 , -0.018801212]]]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant_bias2.conv.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_shape = model_quant_bias2.conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 9, 9])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(model_quant_bias2.conv.weight.detach().numpy(), (weight_shape[2], weight_shape[3], weight_shape[1], weight_shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(input_fp32[0].detach().numpy(), (input_fp32.shape[2], input_fp32.shape[3], input_fp32.shape[1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manual = convolve(np.reshape(input_fp32[0].detach().numpy(), (input_fp32.shape[2], input_fp32.shape[3], input_fp32.shape[1])), np.reshape(model_quant_bias2.conv.weight.detach().numpy(), (weight_shape[2], weight_shape[3], weight_shape[1], weight_shape[0])), model_quant_bias2.conv.bias.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9, 9)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(output_manual, (output_manual.shape[2], output_manual.shape[0], output_manual.shape[1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9, 9)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fp32.detach().numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.008939431290664768, -0.47929776516463996 ,\n",
       "          0.20150431767722532 , -0.2652077691154997  ,\n",
       "         -0.06669273028810441 ,  0.11252881644845192 ,\n",
       "          0.23876764493679614 , -0.02235389768274354 ,\n",
       "         -0.3857006154869582  ],\n",
       "        [-0.41669642515707817 ,  0.33482532251647745 ,\n",
       "         -0.1313713876479965  ,  0.13927097917753484 ,\n",
       "         -0.24526724250558618 ,  0.9736006857956743  ,\n",
       "         -0.2730402102606153  ,  0.3857928696888635  ,\n",
       "         -0.35428114243445985 ],\n",
       "        [-0.043083355577246074, -0.41446137684353945 ,\n",
       "          0.45651978132309196 , -0.3336694059115297  ,\n",
       "          0.2285382187095249  , -0.29916033718485835 ,\n",
       "          0.3345687276350354  , -0.2113290105302581  ,\n",
       "          0.5110127803924256  ],\n",
       "        [-0.5935926826608915  ,  0.3221946724873298  ,\n",
       "         -0.2815999466519523  ,  0.3537310200971195  ,\n",
       "         -0.5959343185042352  ,  0.5816943174923972  ,\n",
       "         -0.6225939321115452  ,  0.833251545330324   ,\n",
       "         -0.7652436448560991  ],\n",
       "        [-0.14779065538808506 , -0.22260439992928371 ,\n",
       "          0.38584163116884906 , -0.5637702810483665  ,\n",
       "          0.5356841663050913  , -0.1292391312159087  ,\n",
       "          0.1478048104668378  , -0.3124769368135103  ,\n",
       "          0.10856743623914178 ],\n",
       "        [-0.6438340942284622  ,  1.089790394151007   ,\n",
       "         -0.7632591867441285  ,  0.45932969249085587 ,\n",
       "         -0.32398858400282937 , -0.32668439372061636 ,\n",
       "         -0.31065496545137483 ,  0.23789138498893792 ,\n",
       "         -0.7068682570846101  ],\n",
       "        [ 0.28755017587936765 , -0.6232901935605708  ,\n",
       "          0.3783926368457146  ,  0.01732044422586121 ,\n",
       "          0.3821827604358366  , -0.0306305229627708  ,\n",
       "          0.25095673544950786 , -0.570333560835119   ,\n",
       "          0.3682421386724208  ],\n",
       "        [-0.7715414083275909  ,  0.4242503651987815  ,\n",
       "         -0.4343247163795121  ,  0.5999478780941188  ,\n",
       "         -0.45704155672463687 ,  0.2743650567169075  ,\n",
       "         -0.6725960461042522  ,  0.5043035997009665  ,\n",
       "         -0.7686933503255611  ],\n",
       "        [-0.3067158435240925  , -0.24270940044724654 ,\n",
       "         -0.018098512847232406, -0.39650054511927224 ,\n",
       "          0.315467041503828   , -0.6719527637068756  ,\n",
       "          0.6216437801975255  , -0.6025732721406145  ,\n",
       "         -0.30870555279089784 ]],\n",
       "\n",
       "       [[-0.3538752408901904  ,  0.309300166294546   ,\n",
       "         -0.4199131787056052  ,  0.5828112002571176  ,\n",
       "         -0.054081859918658104,  0.5689067307614921  ,\n",
       "         -0.09816757153828948 ,  0.4469661565184228  ,\n",
       "         -0.27432549320325283 ],\n",
       "        [ 0.17852615491570267 , -0.24403250053722747 ,\n",
       "          0.6294906874828388  , -0.12548473193261045 ,\n",
       "          0.8448052272724553  , -0.20945723410630612 ,\n",
       "          0.4762347489922376  , -0.4717285983033346  ,\n",
       "          0.48898483770190515 ],\n",
       "        [-0.47532195891535545 ,  0.19582074031765706 ,\n",
       "         -0.2886422890566065  ,  0.3928303199742553  ,\n",
       "         -0.42699429116747556 ,  0.44035629043342545 ,\n",
       "         -0.7268142446731956  ,  0.109285962668426   ,\n",
       "         -0.46562533447656373 ],\n",
       "        [ 0.4033683674398525  , -0.06856568161220045 ,\n",
       "          0.2822665434201101  , -0.3684310094819012  ,\n",
       "          0.5002771792308507  , -0.6222588617011975  ,\n",
       "          0.062241090384276276, -0.34310430108108503 ,\n",
       "          0.43583786286711185 ],\n",
       "        [-0.36677439071700135 ,  0.6414815571918477  ,\n",
       "         -0.5314409711544037  ,  0.7816661696142402  ,\n",
       "         -0.6341281213588779  ,  0.5651585503365844  ,\n",
       "         -0.21439609741975563 ,  0.20822450592183123 ,\n",
       "         -0.3937060626128215  ],\n",
       "        [ 0.7604114364809934  , -0.37196644957040537 ,\n",
       "          0.7061320000110324  , -0.5485306844248927  ,\n",
       "          0.424204174254203   , -0.34497649935879227 ,\n",
       "          0.5112215602099217  , -0.2663693847903459  ,\n",
       "          0.1731035123442486  ],\n",
       "        [-0.5027677234552776  ,  0.43145323470563407 ,\n",
       "         -0.8634961040519697  ,  0.43488291894446246 ,\n",
       "         -0.3999615166392023  ,  0.3831940216007699  ,\n",
       "         -0.1411967606739386  , -0.04979208486581088 ,\n",
       "         -0.4223415589218449  ],\n",
       "        [ 0.35377499962656045 , -0.11281205253522275 ,\n",
       "          0.5618273904019565  , -0.08520287492219558 ,\n",
       "          0.2039629721307425  , -0.08478423913728506 ,\n",
       "          0.4630087131628512  , -0.6557639004359546  ,\n",
       "          0.7784591157432594  ],\n",
       "        [-0.4570043384419651  , -0.09226689301739521 ,\n",
       "         -0.5595355620286055  , -0.05191367902652988 ,\n",
       "         -0.6526733643850184  ,  0.2892487840984605  ,\n",
       "         -0.5368486872886662  ,  0.06656526404559132 ,\n",
       "         -0.4540184573116941  ]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fp32.detach().numpy()[0] - np.reshape(output_manual, (output_manual.shape[2], output_manual.shape[0], output_manual.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 13:09:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1048f3b05e85321c8253e095fcb57a50212287a1649d557864a0f57b3496baef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

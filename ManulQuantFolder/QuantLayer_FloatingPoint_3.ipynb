{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spell.ml/blog/pytorch-quantization-X8e7wBAAACIAHPhT\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan-zhihu-com.translate.goog/p/299108528?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=wapp\n",
    "\n",
    "https://zhuanlan-zhihu-com.translate.goog/p/149659607?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=wapp\n",
    "\n",
    "https://github.com/IntelLabs/distiller/issues/327 -> Quantizing a MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from custom_convolve import convolve_torch\n",
    "torch.set_printoptions(precision=30)\n",
    "np.set_printoptions(precision=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 Floating point model - Full Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_double(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(M_double, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        # self.BN = nn.BatchNorm2d(3)\n",
    "        # self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        # self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1).double()\n",
    "        # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.03307433053851127625, -0.13484150171279907227, -0.21625524759292602539], \n",
    "        #                                                         [ 0.14247404038906097412, -0.14247404038906097412, -0.24932956695556640625], \n",
    "        #                                                         [ 0.32311078906059265137, -0.14501821994781494141, -0.21371106803417205811]]]]))\n",
    "        # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 1.0, 1.0, -1.0], \n",
    "        #                                                         [ -1.0, 1.0, 1.0], \n",
    "        #                                                         [ 1.0, 1.0, -1.0]]]]).double())\n",
    "        # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ -1.0, 0.0, 1.0], \n",
    "        #                                                         [ -1.0, 0.0, 1.0], \n",
    "        #                                                         [ -1.0, 0.0, 1.0]]]]))\n",
    "        # self.conv.bias = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "        # self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M_double(\n",
       "  (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_double = M_double()\n",
    "model_double.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(model_float, model_double):\n",
    "    input_fp32 = torch.rand(1, 1, 300, 300)\n",
    "    \n",
    "    # model_output_float = model_float(input_fp32).detach()\n",
    "    model_output_double = model_double(input_fp32.double()).detach()\n",
    "\n",
    "    # print(\"Input image dtype\", input_fp32.dtype)\n",
    "    # convolved_img_numpy_float32 = convolve_numpy(input_fp32[0].detach().numpy().astype(np.float32), model_float.conv.weight.detach().numpy().astype(np.float32), model_float.conv.bias.detach().numpy().astype(np.float32))\n",
    "    # # print('Numpy float 64')\n",
    "    # convolved_img_numpy_double = convolve_numpy(input_fp32[0].detach().numpy().astype(np.float64), model_float.conv.weight.detach().numpy().astype(np.float64), model_float.conv.bias.detach().numpy().astype(np.float64))\n",
    "    \n",
    "    # difference_numpy_float32 = model_output_float.numpy() - convolved_img_numpy_float32\n",
    "    # difference_numpy_double = model_output_double.numpy() - convolved_img_numpy_double\n",
    "\n",
    "    # print(\"Numpy float model difference :\", np.sum(difference_numpy_float32))\n",
    "    # print(\"Numpy double model difference :\", np.sum(difference_numpy_double))\n",
    "\n",
    "    # convolved_img_torch_float = convolve_torch(input_fp32[0].detach(), model_float.conv.weight.detach(), model_float.conv.bias.detach())\n",
    "    convolved_img_torch_double = convolve_torch(input_fp32[0].detach().double(), model_double.conv.weight.detach().double(), model_double.conv.bias.detach().double())\n",
    "    \n",
    "    # print(\"Outside convolved image shape\", convolved_img.dtype)\n",
    "    # print(\"Model Output type\", model_output.dtype)\n",
    "    \n",
    "    # difference_torch_float32 = model_output_float - convolved_img_torch_float\n",
    "    difference_torch_double = model_output_double - convolved_img_torch_double\n",
    "\n",
    "    # print(\"Torch float model difference :\", torch.sum(difference_torch_float32))\n",
    "    print(\"Torch double model difference :\", torch.sum(difference_torch_double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch double model difference : tensor(1.082467449009527626913040876389e-14, dtype=torch.float64)\n",
      "Iteration : 0\n",
      "Torch double model difference : tensor(9.409140133698201680090278387070e-15, dtype=torch.float64)\n",
      "Iteration : 1\n",
      "Torch double model difference : tensor(2.609024107869117869995534420013e-15, dtype=torch.float64)\n",
      "Iteration : 2\n",
      "Torch double model difference : tensor(2.442490654175344388931989669800e-15, dtype=torch.float64)\n",
      "Iteration : 3\n",
      "Torch double model difference : tensor(2.942091015256664832122623920441e-15, dtype=torch.float64)\n",
      "Iteration : 4\n",
      "Torch double model difference : tensor(-6.911138328291599464137107133865e-15, dtype=torch.float64)\n",
      "Iteration : 5\n",
      "Torch double model difference : tensor(6.855627177060341637115925550461e-15, dtype=torch.float64)\n",
      "Iteration : 6\n",
      "Torch double model difference : tensor(4.107825191113079199567437171936e-15, dtype=torch.float64)\n",
      "Iteration : 7\n",
      "Torch double model difference : tensor(1.321165399303936283104121685028e-14, dtype=torch.float64)\n",
      "Iteration : 8\n",
      "Torch double model difference : tensor(-9.714451465470119728706777095795e-16, dtype=torch.float64)\n",
      "Iteration : 9\n"
     ]
    }
   ],
   "source": [
    "for iter in range(10):\n",
    "    input_fp32 = torch.rand(1, 1, 300, 300)\n",
    "    \n",
    "    # model_output_float = model_float(input_fp32).detach()\n",
    "    model_output_double = model_double(input_fp32.double()).detach()\n",
    "\n",
    "    # convolved_img_torch_float = convolve_torch(input_fp32[0].detach(), model_float.conv.weight.detach(), model_float.conv.bias.detach())\n",
    "    convolved_img_torch_double = convolve_torch(input_fp32[0].detach().double(), model_double.conv.weight.detach().double(), model_double.conv.bias.detach().double())\n",
    "    \n",
    "    # print(\"Outside convolved image shape\", convolved_img.dtype)\n",
    "    # print(\"Model Output type\", model_output.dtype)\n",
    "    \n",
    "    # difference_torch_float32 = model_output_float - convolved_img_torch_float\n",
    "    difference_torch_double = model_output_double - convolved_img_torch_double\n",
    "\n",
    "    # print(\"Torch float model difference :\", torch.sum(difference_torch_float32))\n",
    "    print(\"Torch double model difference :\", torch.sum(difference_torch_double))\n",
    "    \n",
    "    print(\"Iteration :\", iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Stride Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_stride(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(M_stride, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1)\n",
    "        # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 0.03307433053851127625, -0.13484150171279907227, -0.21625524759292602539], \n",
    "        #                                                         [ 0.14247404038906097412, -0.14247404038906097412, -0.24932956695556640625], \n",
    "        #                                                         [ 0.32311078906059265137, -0.14501821994781494141, -0.21371106803417205811]]]]))\n",
    "        # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ 1.0, 1.0, -1.0], \n",
    "        #                                                         [ -1.0, 1.0, 1.0], \n",
    "        #                                                         [ 1.0, 1.0, -1.0]]]]).double())\n",
    "        self.conv.weight = torch.nn.Parameter(torch.tensor([[[[ -1.0, 0.0, 1.0], \n",
    "                                                              [ -1.0, 0.0, 1.0], \n",
    "                                                              [ -1.0, 0.0, 1.0]]]]).double())\n",
    "        self.conv.bias = torch.nn.Parameter(torch.tensor([0.0]).double())\n",
    "        # self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M_stride(\n",
       "  (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_stride = M_stride()\n",
    "model_fp32_stride.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.812963843345642089843750000000,  0.059485256671905517578125000000,\n",
       "           -0.654202759265899658203125000000,  ...,\n",
       "           -0.648304939270019531250000000000,  0.671665370464324951171875000000,\n",
       "           -0.731793582439422607421875000000],\n",
       "          [ 1.422105014324188232421875000000, -0.173482835292816162109375000000,\n",
       "           -0.734119415283203125000000000000,  ...,\n",
       "            0.136144816875457763671875000000, -0.165659725666046142578125000000,\n",
       "           -0.928222298622131347656250000000],\n",
       "          [ 1.116674184799194335937500000000,  0.160289287567138671875000000000,\n",
       "           -0.155205786228179931640625000000,  ...,\n",
       "            0.873344719409942626953125000000, -0.672932207584381103515625000000,\n",
       "           -0.434129238128662109375000000000],\n",
       "          ...,\n",
       "          [ 1.382569432258605957031250000000,  0.005407094955444335937500000000,\n",
       "            0.474898040294647216796875000000,  ...,\n",
       "            0.139550626277923583984375000000, -0.576758146286010742187500000000,\n",
       "            0.610600471496582031250000000000],\n",
       "          [ 1.352222740650177001953125000000,  0.156930387020111083984375000000,\n",
       "            0.836299538612365722656250000000,  ...,\n",
       "            0.140804529190063476562500000000, -0.784388661384582519531250000000,\n",
       "            0.172309815883636474609375000000],\n",
       "          [ 2.055872440338134765625000000000, -1.008474171161651611328125000000,\n",
       "            0.118357539176940917968750000000,  ...,\n",
       "           -1.111817896366119384765625000000,  1.650990247726440429687500000000,\n",
       "           -1.282487750053405761718750000000]]]], dtype=torch.float64,\n",
       "       grad_fn=<SlowConv2DBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_stride(input_fp32.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_stride = convolve_torch(input_fp32[0].detach().double(), model_fp32_stride.conv.weight.detach().double(), model_fp32_stride.conv.bias.detach().double(), stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.812963843345642089843750000000,  0.059485256671905517578125000000,\n",
       "          -0.654202759265899658203125000000,  ...,\n",
       "          -0.648304939270019531250000000000,  0.671665370464324951171875000000,\n",
       "          -0.731793582439422607421875000000],\n",
       "         [ 1.422105014324188232421875000000, -0.173482835292816162109375000000,\n",
       "          -0.734119415283203125000000000000,  ...,\n",
       "           0.136144816875457763671875000000, -0.165659725666046142578125000000,\n",
       "          -0.928222298622131347656250000000],\n",
       "         [ 1.116674184799194335937500000000,  0.160289287567138671875000000000,\n",
       "          -0.155205786228179931640625000000,  ...,\n",
       "           0.873344719409942626953125000000, -0.672932207584381103515625000000,\n",
       "          -0.434129238128662109375000000000],\n",
       "         ...,\n",
       "         [ 1.382569432258605957031250000000,  0.005407094955444335937500000000,\n",
       "           0.474898040294647216796875000000,  ...,\n",
       "           0.139550626277923583984375000000, -0.576758146286010742187500000000,\n",
       "           0.610600471496582031250000000000],\n",
       "         [ 1.352222740650177001953125000000,  0.156930387020111083984375000000,\n",
       "           0.836299538612365722656250000000,  ...,\n",
       "           0.140804529190063476562500000000, -0.784388661384582519531250000000,\n",
       "           0.172309815883636474609375000000],\n",
       "         [ 2.055872440338134765625000000000, -1.008474171161651611328125000000,\n",
       "           0.118357539176940917968750000000,  ...,\n",
       "          -1.111817896366119384765625000000,  1.650990247726440429687500000000,\n",
       "          -1.282487750053405761718750000000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.812963843345642089843750000000,  0.059485256671905517578125000000,\n",
       "           -0.654202759265899658203125000000,  ...,\n",
       "           -0.648304939270019531250000000000,  0.671665370464324951171875000000,\n",
       "           -0.731793582439422607421875000000],\n",
       "          [ 1.422105014324188232421875000000, -0.173482835292816162109375000000,\n",
       "           -0.734119415283203125000000000000,  ...,\n",
       "            0.136144816875457763671875000000, -0.165659725666046142578125000000,\n",
       "           -0.928222298622131347656250000000],\n",
       "          [ 1.116674184799194335937500000000,  0.160289287567138671875000000000,\n",
       "           -0.155205786228179931640625000000,  ...,\n",
       "            0.873344719409942626953125000000, -0.672932207584381103515625000000,\n",
       "           -0.434129238128662109375000000000],\n",
       "          ...,\n",
       "          [ 1.382569432258605957031250000000,  0.005407094955444335937500000000,\n",
       "            0.474898040294647216796875000000,  ...,\n",
       "            0.139550626277923583984375000000, -0.576758146286010742187500000000,\n",
       "            0.610600471496582031250000000000],\n",
       "          [ 1.352222740650177001953125000000,  0.156930387020111083984375000000,\n",
       "            0.836299538612365722656250000000,  ...,\n",
       "            0.140804529190063476562500000000, -0.784388661384582519531250000000,\n",
       "            0.172309815883636474609375000000],\n",
       "          [ 2.055872440338134765625000000000, -1.008474171161651611328125000000,\n",
       "            0.118357539176940917968750000000,  ...,\n",
       "           -1.111817896366119384765625000000,  1.650990247726440429687500000000,\n",
       "           -1.282487750053405761718750000000]]]], dtype=torch.float64,\n",
       "       grad_fn=<SlowConv2DBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_stride(input_fp32.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(result_stride - model_fp32_stride(input_fp32.double())[0].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_fullweight_multichannel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "            super(M_fullweight_multichannel, self).__init__()\n",
    "            # self.BN = nn.BatchNorm2d(3)\n",
    "            # self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "            # self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1).double()\n",
    "            # self.conv.weight = torch.nn.Parameter(torch.zeros(1,2, 3, 3))\n",
    "            # self.conv.weight = torch.nn.Parameter(torch.tensor([[[[0., 0., 0.],\n",
    "            #                                                       [0., 1., 0.],\n",
    "            #                                                       [0., 0., 0.]]],\n",
    "\n",
    "            #                                                       [[[0., 0., 0.],\n",
    "            #                                                         [0., 1., 0.],\n",
    "            #                                                         [0., 0., 0.]]]]).double())                                          \n",
    "            # self.conv.bias = torch.nn.Parameter(torch.tensor([0.0, 0.0]).double())\n",
    "            # self.conv.bias = torch.nn.Parameter(torch.tensor([2.12300]))\n",
    "            # self.relu = torch.nn.ReLU()\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.BN(x)\n",
    "        x = self.conv(x)\n",
    "        # x = self.relu(x)      \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M_fullweight_multichannel(\n",
       "  (conv): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fullweight_multichannel = M_fullweight_multichannel()\n",
    "model_fullweight_multichannel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.044489271938800811767578125000, -0.013404434546828269958496093750,\n",
       "            0.023243753239512443542480468750],\n",
       "          [ 0.015496133826673030853271484375, -0.021780500188469886779785156250,\n",
       "            0.130318447947502136230468750000],\n",
       "          [-0.014340186491608619689941406250, -0.037016220390796661376953125000,\n",
       "            0.007967264391481876373291015625]],\n",
       "\n",
       "         [[-0.169396921992301940917968750000,  0.033638998866081237792968750000,\n",
       "            0.085606716573238372802734375000],\n",
       "          [-0.013384635560214519500732421875, -0.063823670148849487304687500000,\n",
       "           -0.182716190814971923828125000000],\n",
       "          [-0.161564111709594726562500000000,  0.057816185057163238525390625000,\n",
       "            0.186273828148841857910156250000]],\n",
       "\n",
       "         [[-0.025954263284802436828613281250, -0.144208341836929321289062500000,\n",
       "            0.143235027790069580078125000000],\n",
       "          [ 0.073251269757747650146484375000,  0.143819361925125122070312500000,\n",
       "            0.063207425177097320556640625000],\n",
       "          [-0.120125122368335723876953125000, -0.007892107591032981872558593750,\n",
       "            0.178798407316207885742187500000]]],\n",
       "\n",
       "\n",
       "        [[[-0.041113998740911483764648437500,  0.137220785021781921386718750000,\n",
       "           -0.177346125245094299316406250000],\n",
       "          [-0.163395419716835021972656250000, -0.123302541673183441162109375000,\n",
       "           -0.144022971391677856445312500000],\n",
       "          [-0.077871732413768768310546875000,  0.016744054853916168212890625000,\n",
       "           -0.119586028158664703369140625000]],\n",
       "\n",
       "         [[-0.155398190021514892578125000000,  0.014490593224763870239257812500,\n",
       "           -0.174127861857414245605468750000],\n",
       "          [ 0.062128357589244842529296875000,  0.150355532765388488769531250000,\n",
       "            0.047180190682411193847656250000],\n",
       "          [ 0.118057258427143096923828125000, -0.129165947437286376953125000000,\n",
       "           -0.002166076563298702239990234375]],\n",
       "\n",
       "         [[ 0.085452727973461151123046875000,  0.124845996499061584472656250000,\n",
       "            0.030118228867650032043457031250],\n",
       "          [-0.166599571704864501953125000000,  0.058470144867897033691406250000,\n",
       "            0.014803152531385421752929687500],\n",
       "          [ 0.171403244137763977050781250000,  0.189629971981048583984375000000,\n",
       "           -0.034417115151882171630859375000]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fullweight_multichannel.conv.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fullweight_multichannel.conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp32 = torch.rand(1, 3, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_double = model_fullweight_multichannel(input_fp32.double()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_img_torch_double = convolve_torch(input_fp32[0].detach().double(), model_fullweight_multichannel.conv.weight.detach().double(), model_fullweight_multichannel.conv.bias.detach().double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch double model difference : tensor(-2.386979502944086561910808086395e-15, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch double model difference :\", torch.sum(model_output_double - convolved_img_torch_double))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: With image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "# Reading the image\n",
    "#img = skimage.io.imread(\"fruits2.png\")\n",
    "img = skimage.data.chelsea()\n",
    "img = img/255;\n",
    "# Converting the image into gray.\n",
    "# img = skimage.color.rgb2gray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.tensor(img).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 451, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.560784313725490202173773468530, 0.470588235294117640528099855146,\n",
       "           0.407843137254901955124353207793,  ...,\n",
       "           0.431372549019607864906333816180, 0.305882352941176494098840521474,\n",
       "           0.619607843137254921117573758238],\n",
       "          [0.439215686274509797826226531470, 0.337254901960784336800713845150,\n",
       "           0.599999999999999977795539507497,  ...,\n",
       "           0.298039215686274505667796574926, 0.623529411764705887577520115883,\n",
       "           0.470588235294117640528099855146],\n",
       "          [0.317647058823529393478679594409, 0.647058823529411797359500724269,\n",
       "           0.494117647058823550310080463532,  ...,\n",
       "           0.176470588235294129075825253494, 0.105882352941176469118822467408,\n",
       "           0.050980392156862744390544150974],\n",
       "          ...,\n",
       "          [0.756862745098039191304906125879, 0.678431372549019640061374047946,\n",
       "           0.682352941176470606521320405591,  ...,\n",
       "           0.454901960784313719177163193308, 0.219607843137254898913113265735,\n",
       "           0.592156862745098044875646792207],\n",
       "          [0.458823529411764685637109550953, 0.219607843137254898913113265735,\n",
       "           0.584313725490196111955754076916,  ...,\n",
       "           0.423529411764705876475289869632, 0.678431372549019640061374047946,\n",
       "           0.537254901960784292391792860144],\n",
       "          [0.411764705882352921584299565438, 0.737254901960784359005174337653,\n",
       "           0.584313725490196111955754076916,  ...,\n",
       "           0.541176470588235258851739217789, 0.450980392156862752717216835663,\n",
       "           0.427450980392156842935236227277]],\n",
       "\n",
       "         [[0.749019607843137258385013410589, 0.670588235294117596119178870140,\n",
       "           0.674509803921568673601427690301,  ...,\n",
       "           0.470588235294117640528099855146, 0.258823529411764730046030535959,\n",
       "           0.584313725490196111955754076916],\n",
       "          [0.462745098039215707608207139856, 0.247058823529411775155040231766,\n",
       "           0.588235294117647078415700434562,  ...,\n",
       "           0.443137254901960764286172889115, 0.698039215686274472361105836171,\n",
       "           0.556862745098039235713827110885],\n",
       "          [0.431372549019607864906333816180, 0.698039215686274472361105836171,\n",
       "           0.545098039215686225311685575434,  ...,\n",
       "           0.529411764705882359471900144854, 0.450980392156862752717216835663,\n",
       "           0.415686274509803943555397154341],\n",
       "          ...,\n",
       "          [0.541176470588235258851739217789, 0.403921568627450988664406850148,\n",
       "           0.278431372549019617856913555443,  ...,\n",
       "           0.470588235294117640528099855146, 0.349019607843137236180552918086,\n",
       "           0.670588235294117596119178870140],\n",
       "          [0.498039215686274516770026821177, 0.360784313725490191071543222279,\n",
       "           0.682352941176470606521320405591,  ...,\n",
       "           0.152941176470588247049420260737, 0.498039215686274516770026821177,\n",
       "           0.309803921568627460558786879119],\n",
       "          [0.160784313725490207724888591656, 0.498039215686274516770026821177,\n",
       "           0.309803921568627460558786879119,  ...,\n",
       "           0.745098039215686291925067052944, 0.650980392156862763819447081914,\n",
       "           0.635294117647058786957359188818]],\n",
       "\n",
       "         [[0.545098039215686225311685575434, 0.407843137254901955124353207793,\n",
       "           0.290196078431372572747903859636,  ...,\n",
       "           0.454901960784313719177163193308, 0.333333333333333314829616256247,\n",
       "           0.635294117647058786957359188818],\n",
       "          [0.454901960784313719177163193308, 0.325490196078431381909723540957,\n",
       "           0.666666666666666629659232512495,  ...,\n",
       "           0.133333333333333331482961625625, 0.486274509803921561879036516984,\n",
       "           0.317647058823529393478679594409],\n",
       "          [0.152941176470588247049420260737, 0.529411764705882359471900144854,\n",
       "           0.352941176470588258151650506989,  ...,\n",
       "           0.749019607843137258385013410589, 0.650980392156862763819447081914,\n",
       "           0.631372549019607820497412831173],\n",
       "          ...,\n",
       "          [0.545098039215686225311685575434, 0.403921568627450988664406850148,\n",
       "           0.278431372549019617856913555443,  ...,\n",
       "           0.513725490196078382609812251758, 0.450980392156862752717216835663,\n",
       "           0.647058823529411797359500724269],\n",
       "          [0.501960784313725483229973178823, 0.439215686274509797826226531470,\n",
       "           0.654901960784313730279393439559,  ...,\n",
       "           0.301960784313725472127742932571, 0.529411764705882359471900144854,\n",
       "           0.407843137254901955124353207793],\n",
       "          [0.294117647058823539207850217281, 0.537254901960784292391792860144,\n",
       "           0.415686274509803943555397154341,  ...,\n",
       "           0.635294117647058786957359188818, 0.541176470588235258851739217789,\n",
       "           0.501960784313725483229973178823]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.resize_(1, img.shape[2], img.shape[0], img.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 300, 451])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.009264791328643717793767109470,  0.020349141884572818916865344363,\n",
       "            0.247532056323970017785995878512,  ...,\n",
       "            0.362966044249569352864170923567,  0.312273167569265985843429689339,\n",
       "           -0.086619392947608009336590839666],\n",
       "          [ 0.314880183421353598838265952509,  0.281343919261858310676416294882,\n",
       "           -0.384865522588665076586522673097,  ...,\n",
       "            0.076644306999684719272636357346, -0.129412370510200047668547540525,\n",
       "           -0.053456211045836588002799771857],\n",
       "          [ 0.127945480619844564884601822996,  0.013465282656756338841574915932,\n",
       "            0.308128685710179650580897714462,  ...,\n",
       "            0.130484165896261106176723387762,  0.120841194782762062498449040504,\n",
       "           -0.214429652549148752616048341224],\n",
       "          ...,\n",
       "          [ 0.416846653738015715529030558173,  0.054413661788379252826075571647,\n",
       "           -0.187454820082911310930029458177,  ...,\n",
       "            0.027758782060248454737205747733, -0.171456497626673476375458449183,\n",
       "           -0.446142500976088929309781860866],\n",
       "          [ 0.317471420884792077110603258916,  0.304518859736767122381451144975,\n",
       "            0.222272012869850432714713406313,  ...,\n",
       "           -0.026968651438287860599984924193,  0.089957831483822192630839253980,\n",
       "           -0.052518290224860264547857013895],\n",
       "          [-0.084440571465290159558492177894, -0.015576048982033352530152114923,\n",
       "            0.012555592692265971077603126105,  ...,\n",
       "            0.025992534500282993281672361263, -0.104987129717692084795999107882,\n",
       "           -0.261977516262689058557100452163]],\n",
       "\n",
       "         [[-0.140061244799273898475888699977, -0.055135494487694225451690499540,\n",
       "           -0.135625122938940856354150810148,  ...,\n",
       "           -0.157375209550454253637497004092, -0.046136469991128359335874620228,\n",
       "            0.039521698348005784851011412684],\n",
       "          [-0.317270852333038866355252594076, -0.356142077352717856264519014076,\n",
       "           -0.354705867148308429115388662467,  ...,\n",
       "            0.031878760148668028318041933744,  0.061251146500295616803555276420,\n",
       "            0.166070752902705898179647192592],\n",
       "          [-0.061989318255661340906215173163, -0.014551605671003386976991578194,\n",
       "           -0.277883595595311450043851664304,  ...,\n",
       "           -0.222765078712051367570978754884, -0.169143971599479747691674447196,\n",
       "            0.097432304937683955614602382411],\n",
       "          ...,\n",
       "          [ 0.116043424036623543660340374117,  0.210923028215075969438174752213,\n",
       "            0.102921132737287090996858296421,  ...,\n",
       "            0.104314923538121551160884337150, -0.292573037678619585211237108524,\n",
       "            0.360162908030675410486765031237],\n",
       "          [-0.020358305206932658659724211248, -0.545790201452119871916579540994,\n",
       "           -0.276739771568822623404315663720,  ...,\n",
       "           -0.172629510891612492073932116909, -0.343107631795869616464500495567,\n",
       "           -0.209058976050164213589255268744],\n",
       "          [ 0.127998304998007461641407189745,  0.051209026378414956859330686711,\n",
       "            0.041122281444650132797846708854,  ...,\n",
       "           -0.242657194396386666213061289454, -0.212553507924323592526150150661,\n",
       "            0.042569540997232646617476348183]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fullweight_multichannel(input_fp32.double()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_img_torch_double = convolve_torch(input_fp32[0].detach().double(), model_fullweight_multichannel.conv.weight.detach().double(), model_fullweight_multichannel.conv.bias.detach().double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch double model difference : tensor(-2.386979502944086561910808086395e-15, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch double model difference :\", torch.sum(model_output_double - convolved_img_torch_double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1048f3b05e85321c8253e095fcb57a50212287a1649d557864a0f57b3496baef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 13:09:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

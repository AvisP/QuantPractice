{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\n",
    "DataSet downloading and extracting https://byul91oh.tistory.com/325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb982fb4130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Setup warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the MobileNetV2 model architecture, with several notable modifications to enable quantization:\n",
    "\n",
    "* Replacing addition with nn.quantized.FloatFunctional\n",
    "* Insert QuantStub and DeQuantStub at the beginning and end of the network.\n",
    "* Replace ReLU6 with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
    "            # Replace with ReLU\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup, momentum=0.1),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        # Replace torch.add with floatfunctional\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            # return x+self.conv(x)\n",
    "            return self.skip_add.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.quant(x)\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n",
    "    # This operation does not change the numerics\n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == ConvBNReLU:\n",
    "                torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "            if type(m) == InvertedResidual:\n",
    "                for idx in range(len(m.conv)):\n",
    "                    if type(m.conv[idx]) == nn.Conv2d:\n",
    "                        torch.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def load_model(model_file):\n",
    "    model = MobileNetV2()\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loaders(data_path):\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    dataset = dataset_test = torchvision.datasets.ImageNet(\n",
    "          data_path, split=\"train\",\n",
    "              transform = transforms.Compose([\n",
    "                  transforms.Resize(256),\n",
    "                  transforms.CenterCrop(224),\n",
    "                  transforms.ToTensor(),\n",
    "                  normalize,\n",
    "              ]))\n",
    "    dataset_test = torchvision.datasets.ImageNet(\n",
    "          data_path, split=\"val\",\n",
    "              transform = transforms.Compose([\n",
    "                  transforms.Resize(256),\n",
    "                  transforms.CenterCrop(224),\n",
    "                  transforms.ToTensor(),\n",
    "                  normalize,\n",
    "              ]))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test\n",
    "    # return data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/dataset/imagenet/'\n",
    "saved_model_dir = '/home/avishek/Quantization/model_weights/'\n",
    "float_model_file = 'mobilenet_pretrained_float.pth'\n",
    "scripted_float_model_file = 'mobilenet_quantization_scripted.pth'\n",
    "scripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n",
    "\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader, data_loader_test = prepare_data_loaders(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-26 00:32:39--  https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 13.225.185.65, 13.225.185.5, 13.225.185.84, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|13.225.185.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14212972 (14M) [binary/octet-stream]\n",
      "Saving to: ‘mobilenet_v2-b0353104.pth’\n",
      "\n",
      "mobilenet_v2-b03531 100%[===================>]  13.55M  32.6MB/s    in 0.4s    \n",
      "\n",
      "2021-12-26 00:32:40 (32.6 MB/s) - ‘mobilenet_v2-b0353104.pth’ saved [14212972/14212972]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://download.pytorch.org/models/mobilenet_v2-b0353104.pth  \n",
    "## Add a section for renaming the model and moving to a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inverted Residual Block: Before fusion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      " Inverted Residual Block: After fusion\n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n",
    "# while also improving numerical accuracy. While this can be used with any model, this is\n",
    "# especially common with quantized models.\n",
    "\n",
    "print('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\n",
    "float_model.eval()\n",
    "\n",
    "# Fuses modules\n",
    "float_model.fuse_model()\n",
    "\n",
    "# Note fusion of Conv+BN+Relu and Conv+Relu\n",
    "print('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 13.999657\n",
      ".......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Evaluation accuracy on 50000 images, 71.82\n"
     ]
    }
   ],
   "source": [
    "num_eval_batches = 1000\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " Inverted Residual Block:After observer insertion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): ReLU(\n",
      "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(\n",
      "    32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/quantization/observer.py:121: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 32\n",
    "\n",
    "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "myModel.eval()\n",
    "\n",
    "# Fuse Conv, bn and relu\n",
    "myModel.fuse_model()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print(myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate first\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "print('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/quantization/observer.py:243: UserWarning: must run observer before calling calculate_qparams.                                        Returning default scale and zero point \n",
      "  Returning default scale and zero point \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Convert done\n",
      "\n",
      " Inverted Residual Block: After fusion and quantization, note fused modules: \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.13420675694942474, zero_point=0, padding=(1, 1), groups=32)\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.18153458833694458, zero_point=68)\n",
      "  (2): Identity()\n",
      ")\n",
      "Size of model after quantization\n",
      "Size (MB): 3.631847\n"
     ]
    }
   ],
   "source": [
    "# Calibrate with the training set\n",
    "evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Evaluation accuracy on 50000 images, 56.94\n"
     ]
    }
   ],
   "source": [
    "### Accuracy of quantized model on validation Set\n",
    "top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this quantized model, we see an accuracy of 56.7% on the eval dataset. This is because we used a simple min/max observer to determine quantization parameters. Nevertheless, we did reduce the size of our model down to just under 3.6 MB, almost a 4x decrease.\n",
    "\n",
    "In addition, we can significantly improve on the accuracy simply by using a different quantization configuration. We repeat the same exercise with the recommended configuration for quantizing for x86 architectures. This configuration does the following:\n",
    "\n",
    "Quantizes weights on a per-channel basis\n",
    "Uses a histogram observer that collects a histogram of activations and then picks quantization parameters in an optimal manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
      "................................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/quantization/observer.py:990: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  Returning default scale and zero point \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): QuantizedConvReLU2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.023463362827897072, zero_point=0, padding=(1, 1))\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.08709785342216492, zero_point=0, padding=(1, 1), groups=32)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.09489655494689941, zero_point=65)\n",
       "        (2): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=1.0, zero_point=0\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.038721561431884766, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=0.027145760133862495, zero_point=0, padding=(1, 1), groups=96)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.06204567104578018, zero_point=56)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=1.0, zero_point=0\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.010025504045188427, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.014965841546654701, zero_point=0, padding=(1, 1), groups=144)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.06099696457386017, zero_point=62)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.08581560850143433, zero_point=59\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.015225919894874096, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(144, 144, kernel_size=(3, 3), stride=(2, 2), scale=0.019717002287507057, zero_point=0, padding=(1, 1), groups=144)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.048625215888023376, zero_point=69)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=1.0, zero_point=0\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.009249387308955193, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.011279958300292492, zero_point=0, padding=(1, 1), groups=192)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.03913591802120209, zero_point=66)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.05903790518641472, zero_point=65\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.008265032432973385, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.012542624957859516, zero_point=0, padding=(1, 1), groups=192)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.036378346383571625, zero_point=66)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.06776746362447739, zero_point=66\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.011648496612906456, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(192, 192, kernel_size=(3, 3), stride=(2, 2), scale=0.01723942719399929, zero_point=0, padding=(1, 1), groups=192)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.03958079218864441, zero_point=64)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=1.0, zero_point=0\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.006397368386387825, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.009410260245203972, zero_point=0, padding=(1, 1), groups=384)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.029890872538089752, zero_point=66)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.04284103214740753, zero_point=64\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.005188420880585909, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.009023490361869335, zero_point=0, padding=(1, 1), groups=384)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.023609643802046776, zero_point=68)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.044282786548137665, zero_point=65\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.00550643727183342, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.02017192356288433, zero_point=0, padding=(1, 1), groups=384)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.04976728931069374, zero_point=76)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.05941636487841606, zero_point=70\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.008719510398805141, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.01743595115840435, zero_point=0, padding=(1, 1), groups=384)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.03546874597668648, zero_point=63)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=1.0, zero_point=0\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.0081776799634099, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.015756947919726372, zero_point=0, padding=(1, 1), groups=576)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.047138966619968414, zero_point=61)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.05097455158829689, zero_point=61\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.01027094665914774, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.016444219276309013, zero_point=0, padding=(1, 1), groups=576)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.10565601289272308, zero_point=67)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.10914597660303116, zero_point=64\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.02245267666876316, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(576, 576, kernel_size=(3, 3), stride=(2, 2), scale=0.047173872590065, zero_point=0, padding=(1, 1), groups=576)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.1394861489534378, zero_point=62)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=1.0, zero_point=0\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.026158981025218964, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.04308765009045601, zero_point=0, padding=(1, 1), groups=960)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.11540674418210983, zero_point=57)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.19720788300037384, zero_point=60\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.033133599907159805, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.045816633850336075, zero_point=0, padding=(1, 1), groups=960)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.2649243175983429, zero_point=68)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=0.4397538900375366, zero_point=61\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.023370513692498207, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): QuantizedConvReLU2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.011359470896422863, zero_point=0, padding=(1, 1), groups=960)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizedConv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), scale=0.02429961785674095, zero_point=63)\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): QFunctional(\n",
       "        scale=1.0, zero_point=0\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): QuantizedConvReLU2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=0.09659042209386826, zero_point=0)\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): QuantizedLinear(in_features=1280, out_features=1000, scale=0.32397088408470154, zero_point=38, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
    "per_channel_quantized_model.eval()\n",
    "per_channel_quantized_model.fuse_model()\n",
    "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(per_channel_quantized_model, inplace=True)\n",
    "evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\n",
    "torch.quantization.convert(per_channel_quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "### Accuracy of post training quantized model on validation Set\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization-aware training (QAT) is the quantization method that typically results in the highest accuracy. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method will usually yield higher accuracy than either dynamic quantization or post-training static quantization.\n",
    "\n",
    "The overall workflow for actually performing QAT is very similar to before:\n",
    "\n",
    "    We can use the same model as before: there is no additional preparation needed for quantization-aware training.\n",
    "    We need to use a qconfig specifying what kind of fake-quantization is to be inserted after weights and activations, instead of specifying observers\n",
    "\n",
    "We first define a training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
    "    model.train()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    avgloss = AverageMeter('Loss', '1.5f')\n",
    "\n",
    "    cnt = 0\n",
    "    for image, target in data_loader:\n",
    "        start_time = time.time()\n",
    "        print('.', end = '')\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "        avgloss.update(loss, image.size(0))\n",
    "        if cnt >= ntrain_batches:\n",
    "            print('Loss', avgloss.avg)\n",
    "\n",
    "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "                  .format(top1=top1, top5=top5))\n",
    "            return\n",
    "\n",
    "    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model = load_model(saved_model_dir + float_model_file)\n",
    "qat_model.fuse_model()\n",
    "\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
    "torch.backends.quantized.engine = 'qnnpack'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, prepare_qat performs the “fake quantization”, preparing the model for quantization-aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After preparation for QAT, note fake-quantization modules \n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvBnReLU2d(\n",
      "      32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1,         scale=tensor([1.]), zero_point=tensor([0])\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (activation_post_process): FakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1,         scale=tensor([1.]), zero_point=tensor([0])\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): ConvBn2d(\n",
      "    32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (weight_fake_quant): FakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1,         scale=tensor([1.]), zero_point=tensor([0])\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (activation_post_process): FakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8),            quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1,         scale=tensor([1.]), zero_point=tensor([0])\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a quantized model with high accuracy requires accurate modeling of numerics at inference. For quantization aware training, therefore, we modify the training loop by:\n",
    "\n",
    "    Switch batch norm to use running mean and variance towards the end of training to better match inference numerics.\n",
    "    We also freeze the quantizer parameters (scale and zero-point) and fine tune the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Loss tensor(0.9131, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 75.667 Acc@5 93.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/quantization/observer.py:243: UserWarning: must run observer before calling calculate_qparams.                                        Returning default scale and zero point \n",
      "  Returning default scale and zero point \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................Epoch 0 :Evaluation accuracy on 2500 images, 77.44\n",
      "....................Loss tensor(0.8492, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 76.167 Acc@5 93.167\n",
      "..................................................Epoch 1 :Evaluation accuracy on 2500 images, 74.52\n",
      "....................Loss tensor(0.8451, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 76.667 Acc@5 94.667\n",
      "..................................................Epoch 2 :Evaluation accuracy on 2500 images, 77.92\n",
      "....................Loss tensor(0.9155, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 77.167 Acc@5 93.500\n",
      "..................................................Epoch 3 :Evaluation accuracy on 2500 images, 77.76\n",
      "....................Loss tensor(0.6210, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 82.667 Acc@5 96.000\n",
      "..................................................Epoch 4 :Evaluation accuracy on 2500 images, 78.00\n",
      "....................Loss tensor(0.8257, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 76.833 Acc@5 95.167\n",
      "..................................................Epoch 5 :Evaluation accuracy on 2500 images, 78.24\n",
      "....................Loss tensor(0.7408, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 79.333 Acc@5 95.500\n",
      "..................................................Epoch 6 :Evaluation accuracy on 2500 images, 78.28\n",
      "....................Loss tensor(0.7831, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 80.000 Acc@5 93.833\n",
      "..................................................Epoch 7 :Evaluation accuracy on 2500 images, 78.00\n"
     ]
    }
   ],
   "source": [
    "num_train_batches = 20\n",
    "num_eval_batches = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# QAT takes time and one needs to train over a few epochs.\n",
    "# Train and check accuracy after each epoch\n",
    "for nepoch in range(8):\n",
    "    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n",
    "    if nepoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        qat_model.apply(torch.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    # Check the accuracy after each epoch\n",
    "    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
    "    quantized_model.eval()\n",
    "    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization-aware training yields an accuracy of over 71.5% on the entire imagenet dataset, which is close to the floating point accuracy of 71.9%.\n",
    "\n",
    "More on quantization-aware training:\n",
    "\n",
    "    QAT is a super-set of post training quant techniques that allows for more debugging. For example, we can analyze if the accuracy of the model is limited by weight or activation quantization.\n",
    "    We can also simulate the accuracy of a quantized model in floating point since we are using fake-quantization to model the numerics of actual quantized arithmetic.\n",
    "    We can mimic post training quantization easily too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedup from quantization\n",
    "\n",
    "Finally, let’s confirm something we alluded to above: do our quantized models actually perform inference faster? Let’s test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  15 ms\n",
      "Elapsed time:   9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.331655740737915"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n",
    "\n",
    "run_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this on a server yielded 1 ms for the regular model 15ms, and just 9 ms for the quantized model, illustrating the typical 2-4x speedup we see for quantized models compared to floating point ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ac3bbeb357d502fdd2d0ff5bef79451109bdd8a25a09fbb1f3c81c870d5065"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

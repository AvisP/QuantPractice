{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://visualstudiomagazine.com/articles/2020/09/10/pytorch-dataloader.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fee98039350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Setup warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "from facenet_pytorch.models.utils.download import download_url_to_file\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_planes, out_planes,\n",
    "            kernel_size=kernel_size, stride=stride,\n",
    "            padding=padding, bias=False\n",
    "        ) # verify bias false\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_planes,\n",
    "            eps=0.001, # value found in tensorflow\n",
    "            momentum=0.1, # default pytorch value\n",
    "            affine=True\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block35(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = BasicConv2d(256, 32, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(256, 32, kernel_size=1, stride=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(256, 32, kernel_size=1, stride=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(96, 256, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.ff1 = nn.quantized.FloatFunctional()\n",
    "        self.ff2 = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "         # out = out * self.scale + x        \n",
    "        # out = self.ff1.mul(torch.tensor(out), torch.tensor(self.scale))\n",
    "        # out = self.ff2.add(out, x)\n",
    "        out = self.conv2d(out)\n",
    "        out = self.ff2.add(self.ff1.mul_scalar(out, self.scale), x) \n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block17(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = BasicConv2d(896, 128, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(896, 128, kernel_size=1, stride=1),\n",
    "            BasicConv2d(128, 128, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            BasicConv2d(128, 128, kernel_size=(7,1), stride=1, padding=(3,0))\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(256, 896, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.ff1 = nn.quantized.FloatFunctional()\n",
    "        self.ff2 = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = self.ff2.add(self.ff1.mul_scalar(out, self.scale), x) \n",
    "        # out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block8(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0, noReLU=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.noReLU = noReLU\n",
    "\n",
    "        self.branch0 = BasicConv2d(1792, 192, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(1792, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=(1,3), stride=1, padding=(0,1)),\n",
    "            BasicConv2d(192, 192, kernel_size=(3,1), stride=1, padding=(1,0))\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)\n",
    "        if not self.noReLU:\n",
    "            self.relu = nn.ReLU(inplace=False)\n",
    "        self.ff1 = nn.quantized.FloatFunctional()\n",
    "        self.ff2 = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = self.ff2.add(self.ff1.mul_scalar(out, self.scale), x) \n",
    "        # out = out * self.scale + x\n",
    "        if not self.noReLU:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_6a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch0 = BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(256, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(192, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_7a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InceptionResnetV1(nn.Module):\n",
    "    \"\"\"Inception Resnet V1 model with optional loading of pretrained weights.\n",
    "    Model parameters can be loaded based on pretraining on the VGGFace2 or CASIA-Webface\n",
    "    datasets. Pretrained state_dicts are automatically downloaded on model instantiation if\n",
    "    requested and cached in the torch cache. Subsequent instantiations use the cache rather than\n",
    "    redownloading.\n",
    "    Keyword Arguments:\n",
    "        pretrained {str} -- Optional pretraining dataset. Either 'vggface2' or 'casia-webface'.\n",
    "            (default: {None})\n",
    "        classify {bool} -- Whether the model should output classification probabilities or feature\n",
    "            embeddings. (default: {False})\n",
    "        num_classes {int} -- Number of output classes. If 'pretrained' is set and num_classes not\n",
    "            equal to that used for the pretrained model, the final linear layer will be randomly\n",
    "            initialized. (default: {None})\n",
    "        dropout_prob {float} -- Dropout probability. (default: {0.6})\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=None, classify=False, num_classes=None, dropout_prob=0.6, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set simple attributes\n",
    "        self.pretrained = pretrained\n",
    "        self.classify = classify\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if pretrained == 'vggface2':\n",
    "            tmp_classes = 8631\n",
    "        elif pretrained == 'casia-webface':\n",
    "            tmp_classes = 10575\n",
    "        elif pretrained is None and self.classify and self.num_classes is None:\n",
    "            raise Exception('If \"pretrained\" is not specified and \"classify\" is True, \"num_classes\" must be specified')\n",
    "\n",
    "\n",
    "        # Define layers\n",
    "        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n",
    "        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n",
    "        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n",
    "        self.conv2d_4b = BasicConv2d(192, 256, kernel_size=3, stride=2)\n",
    "        self.repeat_1 = nn.Sequential(\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "        )\n",
    "        self.mixed_6a = Mixed_6a()\n",
    "        self.repeat_2 = nn.Sequential(\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "        )\n",
    "        self.mixed_7a = Mixed_7a()\n",
    "        self.repeat_3 = nn.Sequential(\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "        )\n",
    "        self.block8 = Block8(noReLU=True)\n",
    "        self.avgpool_1a = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.last_linear = nn.Linear(1792, 512, bias=False)\n",
    "        self.last_bn = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.logits = nn.Linear(512, tmp_classes)\n",
    "            load_weights(self, pretrained)\n",
    "\n",
    "        if self.classify and self.num_classes is not None:\n",
    "            self.logits = nn.Linear(512, self.num_classes)\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "            self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate embeddings or logits given a batch of input image tensors.\n",
    "        Arguments:\n",
    "            x {torch.tensor} -- Batch of image tensors representing faces.\n",
    "        Returns:\n",
    "            torch.tensor -- Batch of embedding vectors or multinomial logits.\n",
    "        \"\"\"\n",
    "        x = self.quant(x)\n",
    "        x = self.conv2d_1a(x)\n",
    "        x = self.conv2d_2a(x)\n",
    "        x = self.conv2d_2b(x)\n",
    "        x = self.maxpool_3a(x)\n",
    "        x = self.conv2d_3b(x)\n",
    "        x = self.conv2d_4a(x)\n",
    "        x = self.conv2d_4b(x)\n",
    "        x = self.repeat_1(x)\n",
    "        x = self.mixed_6a(x)\n",
    "        x = self.repeat_2(x)\n",
    "        x = self.mixed_7a(x)\n",
    "        x = self.repeat_3(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.avgpool_1a(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_linear(x.view(x.shape[0], -1))\n",
    "        x = self.last_bn(x)        \n",
    "        if self.classify:\n",
    "            x = self.logits(x)\n",
    "            x = self.dequant(x)\n",
    "        else:\n",
    "            x = self.dequant(x)\n",
    "            x = F.normalize(x, p=2, dim=1)\n",
    "        # x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == BasicConv2d:\n",
    "                torch.quantization.fuse_modules(m, ['conv', 'bn', 'relu'], inplace=True)\n",
    "        torch.quantization.fuse_modules(self, [[\"last_linear\", \"last_bn\"]], inplace=True)\n",
    "\n",
    "\n",
    "def load_weights(mdl, name):\n",
    "    \"\"\"Download pretrained state_dict and load into model.\n",
    "    Arguments:\n",
    "        mdl {torch.nn.Module} -- Pytorch model.\n",
    "        name {str} -- Name of dataset that was used to generate pretrained state_dict.\n",
    "    Raises:\n",
    "        ValueError: If 'pretrained' not equal to 'vggface2' or 'casia-webface'.\n",
    "    \"\"\"\n",
    "    if name == 'vggface2':\n",
    "        path = 'https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180402-114759-vggface2.pt'\n",
    "    elif name == 'casia-webface':\n",
    "        path = 'https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180408-102900-casia-webface.pt'\n",
    "    else:\n",
    "        raise ValueError('Pretrained models only exist for \"vggface2\" and \"casia-webface\"')\n",
    "\n",
    "    model_dir = os.path.join(get_torch_home(), 'checkpoints')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    cached_file = os.path.join(model_dir, os.path.basename(path))\n",
    "    if not os.path.exists(cached_file):\n",
    "        download_url_to_file(path, cached_file)\n",
    "\n",
    "    state_dict = torch.load(cached_file)\n",
    "    mdl.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_torch_home():\n",
    "    torch_home = os.path.expanduser(\n",
    "        os.getenv(\n",
    "            'TORCH_HOME',\n",
    "            os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')\n",
    "        )\n",
    "    )\n",
    "    return torch_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_linear QuantizedLinear(in_features=1792, out_features=512, scale=0.018916359171271324, zero_point=59, qscheme=torch.per_tensor_affine)\n"
     ]
    }
   ],
   "source": [
    "# for module_name, module in model_inception_resnet.named_children():\n",
    "#     if module_name == 'last_linear':\n",
    "#         print(module_name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            # output = model(image)\n",
    "            output = model_inception_resnet(transforms.ToTensor()(image).unsqueeze(0))\n",
    "            loss = criterion(output, torch.tensor([target]))\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, torch.tensor([target]), topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], transforms.ToTensor()(image).unsqueeze(0).size(0))\n",
    "            # top1.update(acc1[0], image.size(0))\n",
    "            # top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model_inception_resnet = InceptionResnetV1(pretrained='vggface2', classify=True).eval().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/dataset/imagenet/'\n",
    "saved_model_dir = '/home/avishek/Quantization/model_weights/'\n",
    "float_model_file = 'mobilenet_pretrained_float.pth'\n",
    "scripted_float_model_file = 'mobilenet_quantization_scripted.pth'\n",
    "scripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n",
    "\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "# test_dataset = datasets.ImageFolder('/dataset/VGG_Face2//test_images_aligned')\n",
    "# test_dataset = datasets.ImageFolder('/home/avishek/Quantization/data/test_images_aligned2')\n",
    "# test_dataset.idx_to_class = {i:c for c, i in test_dataset.class_to_idx.items()}\n",
    "# test_loader = DataLoader(test_dataset, collate_fn=collate_fn, num_workers=32)\n",
    "\n",
    "train_dataset = datasets.ImageFolder('/home/avishek/Quantization/data/train_images_aligned2')\n",
    "train_dataset.idx_to_class = {i:c for c, i in train_dataset.class_to_idx.items()}\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_fn, num_workers=32, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.targets[1000:1009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.targets[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligned = []\n",
    "# names = []\n",
    "\n",
    "# for x, y in test_loader:\n",
    "#     # x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "#     # if x_aligned is not None:\n",
    "#     #     print('Face detected with probability: {:8f}'.format(prob))\n",
    "#     aligned.append(transforms.ToTensor()(x))\n",
    "#     names.append(test_loader.dataset.idx_to_class[y])\n",
    "#     # class_id.append(test_loader.dataset.samples[y][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Running on device: {}'.format(device))\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/57237381/runtimeerror-expected-4-dimensional-input-for-4-dimensional-weight-32-3-3-but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_vector = []\n",
    "# names = []\n",
    "\n",
    "# for x, y in test_loader:\n",
    "#     feat_vector.append(model_inception_resnet(transforms.ToTensor()(x).unsqueeze(0)).detach().numpy().reshape(8631, ))\n",
    "#     names.append(test_loader.dataset.idx_to_class[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Block: Before fusion \n",
      "\n",
      " Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "Size (MB): 112.018331\n",
      "\n",
      " Model size before fusion :  None\n",
      "\n",
      " After fusion\n",
      "\n",
      " ConvReLU2d(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (1): ReLU()\n",
      ")\n",
      "Size (MB): 111.714175\n",
      "\n",
      " Model size after fusion :  None\n"
     ]
    }
   ],
   "source": [
    "# print('\\n Block: Before fusion \\n\\n', model_inception_resnet.conv2d_1a.conv)\n",
    "# model_inception_resnet.eval()\n",
    "# print('\\n Model size before fusion : ', print_size_of_model( model_inception_resnet))\n",
    "\n",
    "# # Fuses modules\n",
    "# model_inception_resnet.fuse_model()\n",
    "\n",
    "# # Note fusion of Conv+BN+Relu and Conv+Relu\n",
    "# print('\\n After fusion\\n\\n',model_inception_resnet.conv2d_1a.conv)\n",
    "# # model_inception_resnet.eval()\n",
    "# print('\\n Model size after fusion : ', print_size_of_model( model_inception_resnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1048, 12.8072,  4.7573,  ..., -3.3688, -1.6255, -2.5136]])\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     output = model_inception_resnet(transforms.ToTensor()(image).unsqueeze(0))\n",
    "#     print(output)\n",
    "#     loss = criterion(output, torch.tensor([target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# # input = torch.randn(3, 5, requires_grad=True)\n",
    "# # target = torch.tensor([1, 0, 4])\n",
    "# # output = F.nll_loss(F.log_softmax(input), target)\n",
    "\n",
    "# input = torch.randn(1, 5, requires_grad=True)\n",
    "# target = torch.tensor([1])\n",
    "# output = F.nll_loss(F.log_softmax(input), target)\n",
    "# outputc = criterion(F.log_softmax(input), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 112.006249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................Evaluation accuracy on 5000 images, 85.71\n"
     ]
    }
   ],
   "source": [
    "num_eval_batches = 100\n",
    "model_inception_resnet.eval()\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(model_inception_resnet)\n",
    "\n",
    "top1, top5 = evaluate(model_inception_resnet, criterion, train_loader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "# torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................Evaluation accuracy on 5000 images, 85.71\n"
     ]
    }
   ],
   "source": [
    "model_inception_resnet.fuse_model()\n",
    "\n",
    "top1, top5 = evaluate(model_inception_resnet, criterion, train_loader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/observer.py:172: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " After observer insertion \n",
      "\n",
      " ConvReLU2d(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 32\n",
    "\n",
    "# # myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "# # myModel.eval()\n",
    "\n",
    "# # Fuse Conv, bn and relu\n",
    "# model_inception_resnet.fuse_model()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "model_inception_resnet.qconfig = torch.quantization.default_qconfig\n",
    "print(model_inception_resnet.qconfig)\n",
    "torch.quantization.prepare(model_inception_resnet, inplace=True)\n",
    "\n",
    "# Calibrate first\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "print('\\n After observer insertion \\n\\n', model_inception_resnet.conv2d_1a.conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 112.014939\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(model_inception_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................Post Training Quantization: Calibration done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/ao/quantization/utils.py:156: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Convert done\n",
      "\n",
      " Block: After fusion and quantization, note fused modules: \n",
      "\n",
      " QuantizedConvReLU2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.05539536103606224, zero_point=0)\n",
      "Size of model after quantization\n",
      "Size (MB): 28.247461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W TensorImpl.h:1408] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Calibrate with the training set\n",
    "evaluate(model_inception_resnet, criterion, train_loader, neval_batches=num_calibration_batches)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "torch.quantization.convert(model_inception_resnet, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Block: After fusion and quantization, note fused modules: \\n\\n',model_inception_resnet.conv2d_1a.conv)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(model_inception_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................Evaluation accuracy on 5000 images, 80.00\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\nModule 'Block8' has no attribute 'relu' :\n  File \"/tmp/ipykernel_48096/3826597506.py\", line 137\n        # out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n                  ~~~~~~~~~ <--- HERE\n        return out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m top1, top5 \u001b[39m=\u001b[39m evaluate(model_inception_resnet, criterion, train_loader, neval_batches\u001b[39m=\u001b[39mnum_eval_batches)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEvaluation accuracy on \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m images, \u001b[39m\u001b[39m%2.2f\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(num_eval_batches \u001b[39m*\u001b[39m eval_batch_size, top1\u001b[39m.\u001b[39mavg))\n\u001b[0;32m----> 4\u001b[0m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39msave(torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(model_inception_resnet), saved_model_dir \u001b[39m+\u001b[39m scripted_quantized_model_file)\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py:1257\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=1254'>1255</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=1255'>1256</a>\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=1256'>1257</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49mcreate_script_module(\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=1257'>1258</a>\u001b[0m         obj, torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49minfer_methods_to_compile\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=1258'>1259</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=1260'>1261</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=1261'>1262</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py:451\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=448'>449</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=449'>450</a>\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=450'>451</a>\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py:513\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=509'>510</a>\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=511'>512</a>\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=512'>513</a>\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=514'>515</a>\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=515'>516</a>\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py:587\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=573'>574</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=574'>575</a>\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=575'>576</a>\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=583'>584</a>\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=584'>585</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=585'>586</a>\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=586'>587</a>\u001b[0m init_fn(script_module)\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=588'>589</a>\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=589'>590</a>\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_script.py?line=590'>591</a>\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py:491\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=487'>488</a>\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=488'>489</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=489'>490</a>\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=490'>491</a>\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=492'>493</a>\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=493'>494</a>\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py:517\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=514'>515</a>\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=515'>516</a>\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=516'>517</a>\u001b[0m     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=517'>518</a>\u001b[0m     \u001b[39m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=518'>519</a>\u001b[0m     \u001b[39m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=519'>520</a>\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py:368\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=364'>365</a>\u001b[0m property_defs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mdef_ \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=365'>366</a>\u001b[0m property_rcbs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mresolution_callback \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[0;32m--> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_recursive.py?line=367'>368</a>\u001b[0m concrete_type\u001b[39m.\u001b[39;49m_create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nModule 'Block8' has no attribute 'relu' :\n  File \"/tmp/ipykernel_48096/3826597506.py\", line 137\n        # out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n                  ~~~~~~~~~ <--- HERE\n        return out\n"
     ]
    }
   ],
   "source": [
    "### Accuracy of quantized model on validation Set\n",
    "top1, top5 = evaluate(model_inception_resnet, criterion, train_loader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(model_inception_resnet), saved_model_dir + scripted_quantized_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 73984 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/avishek/Quantization/Quant Inception2.ipynb Cell 25'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000023vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m### Accuracy of quantized model on validation Set\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000023vscode-remote?line=1'>2</a>\u001b[0m top1, top5 \u001b[39m=\u001b[39m evaluate(model_inception_resnet, criterion, train_loader, neval_batches\u001b[39m=\u001b[39;49mnum_eval_batches)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000023vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEvaluation accuracy on \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m images, \u001b[39m\u001b[39m%2.2f\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(num_eval_batches \u001b[39m*\u001b[39m eval_batch_size, top1\u001b[39m.\u001b[39mavg))\n",
      "\u001b[1;32m/home/avishek/Quantization/Quant Inception2.ipynb Cell 5'\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, data_loader, neval_batches)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000004vscode-remote?line=46'>47</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000004vscode-remote?line=47'>48</a>\u001b[0m     \u001b[39mfor\u001b[39;00m image, target \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000004vscode-remote?line=48'>49</a>\u001b[0m         \u001b[39m# output = model(image)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000004vscode-remote?line=49'>50</a>\u001b[0m         output \u001b[39m=\u001b[39m model_inception_resnet(transforms\u001b[39m.\u001b[39;49mToTensor()(image)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000004vscode-remote?line=50'>51</a>\u001b[0m         loss \u001b[39m=\u001b[39m criterion(output, torch\u001b[39m.\u001b[39mtensor([target]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000004vscode-remote?line=51'>52</a>\u001b[0m         cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/avishek/Quantization/Quant Inception2.ipynb Cell 4'\u001b[0m in \u001b[0;36mInceptionResnetV1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=290'>291</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2d_4a(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=291'>292</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2d_4b(x)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=292'>293</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepeat_1(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=293'>294</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_6a(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=294'>295</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepeat_2(x)\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/avishek/Quantization/Quant Inception2.ipynb Cell 4'\u001b[0m in \u001b[0;36mBlock35.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=66'>67</a>\u001b[0m  \u001b[39m# out = out * self.scale + x        \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=67'>68</a>\u001b[0m \u001b[39m# out = self.ff1.mul(torch.tensor(out), torch.tensor(self.scale))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=68'>69</a>\u001b[0m \u001b[39m# out = self.ff2.add(out, x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=69'>70</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2d(out)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=70'>71</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff2\u001b[39m.\u001b[39;49madd_scalar(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff1\u001b[39m.\u001b[39;49mmul_scalar(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale), x) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=71'>72</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22504156454c5f4d4c5f536572766572227d/home/avishek/Quantization/Quant%20Inception2.ipynb#ch0000003vscode-remote?line=72'>73</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/quantized/modules/functional_modules.py:192\u001b[0m, in \u001b[0;36mQFunctional.add_scalar\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/quantized/modules/functional_modules.py?line=190'>191</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_scalar\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, y: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/quantized/modules/functional_modules.py?line=191'>192</a>\u001b[0m     r \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mquantized\u001b[39m.\u001b[39;49madd_scalar(x, y)\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/quantized/modules/functional_modules.py?line=192'>193</a>\u001b[0m     \u001b[39m# Note: this operation is not observed because the observation is not\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/quantized/modules/functional_modules.py?line=193'>194</a>\u001b[0m     \u001b[39m# needed for the quantized op.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/canservers/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/quantized/modules/functional_modules.py?line=194'>195</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m r\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 73984 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "### Accuracy of quantized model on validation Set\n",
    "top1, top5 = evaluate(model_inception_resnet, criterion, train_loader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ac3bbeb357d502fdd2d0ff5bef79451109bdd8a25a09fbb1f3c81c870d5065"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://visualstudiomagazine.com/articles/2020/09/10/pytorch-dataloader.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f24032edcd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Setup warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "from facenet_pytorch.models.utils.download import download_url_to_file\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_planes, out_planes,\n",
    "            kernel_size=kernel_size, stride=stride,\n",
    "            padding=padding, bias=False\n",
    "        ) # verify bias false\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_planes,\n",
    "            eps=0.001, # value found in tensorflow\n",
    "            momentum=0.1, # default pytorch value\n",
    "            affine=True\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block35(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = BasicConv2d(256, 32, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(256, 32, kernel_size=1, stride=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(256, 32, kernel_size=1, stride=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(96, 256, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.ff1 = nn.quantized.FloatFunctional()\n",
    "        self.ff2 = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "         # out = out * self.scale + x        \n",
    "        # out = self.ff1.mul(torch.tensor(out), torch.tensor(self.scale))\n",
    "        # out = self.ff2.add(out, x)\n",
    "        out = self.conv2d(out)\n",
    "        out = self.ff2.add_scalar(self.ff1.mul(out, self.scale), x) \n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block17(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = BasicConv2d(896, 128, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(896, 128, kernel_size=1, stride=1),\n",
    "            BasicConv2d(128, 128, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            BasicConv2d(128, 128, kernel_size=(7,1), stride=1, padding=(3,0))\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(256, 896, kernel_size=1, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = out * self.scale + x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block8(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=1.0, noReLU=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.noReLU = noReLU\n",
    "\n",
    "        self.branch0 = BasicConv2d(1792, 192, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(1792, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=(1,3), stride=1, padding=(0,1)),\n",
    "            BasicConv2d(192, 192, kernel_size=(3,1), stride=1, padding=(1,0))\n",
    "        )\n",
    "\n",
    "        self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)\n",
    "        if not self.noReLU:\n",
    "            self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        out = self.conv2d(out)\n",
    "        out = out * self.scale + x\n",
    "        if not self.noReLU:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_6a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch0 = BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(256, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(192, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_7a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(896, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(256, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InceptionResnetV1(nn.Module):\n",
    "    \"\"\"Inception Resnet V1 model with optional loading of pretrained weights.\n",
    "    Model parameters can be loaded based on pretraining on the VGGFace2 or CASIA-Webface\n",
    "    datasets. Pretrained state_dicts are automatically downloaded on model instantiation if\n",
    "    requested and cached in the torch cache. Subsequent instantiations use the cache rather than\n",
    "    redownloading.\n",
    "    Keyword Arguments:\n",
    "        pretrained {str} -- Optional pretraining dataset. Either 'vggface2' or 'casia-webface'.\n",
    "            (default: {None})\n",
    "        classify {bool} -- Whether the model should output classification probabilities or feature\n",
    "            embeddings. (default: {False})\n",
    "        num_classes {int} -- Number of output classes. If 'pretrained' is set and num_classes not\n",
    "            equal to that used for the pretrained model, the final linear layer will be randomly\n",
    "            initialized. (default: {None})\n",
    "        dropout_prob {float} -- Dropout probability. (default: {0.6})\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=None, classify=False, num_classes=None, dropout_prob=0.6, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set simple attributes\n",
    "        self.pretrained = pretrained\n",
    "        self.classify = classify\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if pretrained == 'vggface2':\n",
    "            tmp_classes = 8631\n",
    "        elif pretrained == 'casia-webface':\n",
    "            tmp_classes = 10575\n",
    "        elif pretrained is None and self.classify and self.num_classes is None:\n",
    "            raise Exception('If \"pretrained\" is not specified and \"classify\" is True, \"num_classes\" must be specified')\n",
    "\n",
    "\n",
    "        # Define layers\n",
    "        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n",
    "        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n",
    "        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n",
    "        self.conv2d_4b = BasicConv2d(192, 256, kernel_size=3, stride=2)\n",
    "        self.repeat_1 = nn.Sequential(\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "            Block35(scale=0.17),\n",
    "        )\n",
    "        self.mixed_6a = Mixed_6a()\n",
    "        self.repeat_2 = nn.Sequential(\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "            Block17(scale=0.10),\n",
    "        )\n",
    "        self.mixed_7a = Mixed_7a()\n",
    "        self.repeat_3 = nn.Sequential(\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "            Block8(scale=0.20),\n",
    "        )\n",
    "        self.block8 = Block8(noReLU=True)\n",
    "        self.avgpool_1a = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.last_linear = nn.Linear(1792, 512, bias=False)\n",
    "        self.last_bn = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.logits = nn.Linear(512, tmp_classes)\n",
    "            load_weights(self, pretrained)\n",
    "\n",
    "        if self.classify and self.num_classes is not None:\n",
    "            self.logits = nn.Linear(512, self.num_classes)\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "            self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate embeddings or logits given a batch of input image tensors.\n",
    "        Arguments:\n",
    "            x {torch.tensor} -- Batch of image tensors representing faces.\n",
    "        Returns:\n",
    "            torch.tensor -- Batch of embedding vectors or multinomial logits.\n",
    "        \"\"\"\n",
    "        x = self.quant(x)\n",
    "        x = self.conv2d_1a(x)\n",
    "        x = self.conv2d_2a(x)\n",
    "        x = self.conv2d_2b(x)\n",
    "        x = self.maxpool_3a(x)\n",
    "        x = self.conv2d_3b(x)\n",
    "        x = self.conv2d_4a(x)\n",
    "        x = self.conv2d_4b(x)\n",
    "        x = self.repeat_1(x)\n",
    "        x = self.mixed_6a(x)\n",
    "        x = self.repeat_2(x)\n",
    "        x = self.mixed_7a(x)\n",
    "        x = self.repeat_3(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.avgpool_1a(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_linear(x.view(x.shape[0], -1))\n",
    "        x = self.last_bn(x)        \n",
    "        if self.classify:\n",
    "            x = self.logits(x)\n",
    "            x = self.dequant(x)\n",
    "        else:\n",
    "            x = self.dequant(x)\n",
    "            x = F.normalize(x, p=2, dim=1)\n",
    "        # x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == BasicConv2d:\n",
    "                torch.quantization.fuse_modules(m, ['conv', 'bn', 'relu'], inplace=True)\n",
    "\n",
    "\n",
    "def load_weights(mdl, name):\n",
    "    \"\"\"Download pretrained state_dict and load into model.\n",
    "    Arguments:\n",
    "        mdl {torch.nn.Module} -- Pytorch model.\n",
    "        name {str} -- Name of dataset that was used to generate pretrained state_dict.\n",
    "    Raises:\n",
    "        ValueError: If 'pretrained' not equal to 'vggface2' or 'casia-webface'.\n",
    "    \"\"\"\n",
    "    if name == 'vggface2':\n",
    "        path = 'https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180402-114759-vggface2.pt'\n",
    "    elif name == 'casia-webface':\n",
    "        path = 'https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180408-102900-casia-webface.pt'\n",
    "    else:\n",
    "        raise ValueError('Pretrained models only exist for \"vggface2\" and \"casia-webface\"')\n",
    "\n",
    "    model_dir = os.path.join(get_torch_home(), 'checkpoints')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    cached_file = os.path.join(model_dir, os.path.basename(path))\n",
    "    if not os.path.exists(cached_file):\n",
    "        download_url_to_file(path, cached_file)\n",
    "\n",
    "    state_dict = torch.load(cached_file)\n",
    "    mdl.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_torch_home():\n",
    "    torch_home = os.path.expanduser(\n",
    "        os.getenv(\n",
    "            'TORCH_HOME',\n",
    "            os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')\n",
    "        )\n",
    "    )\n",
    "    return torch_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            # output = model(image)\n",
    "            output = model_inception_resnet(transforms.ToTensor()(image).unsqueeze(0))\n",
    "            loss = criterion(output, torch.tensor([target]))\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, torch.tensor([target]), topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], transforms.ToTensor()(image).unsqueeze(0).size(0))\n",
    "            # top1.update(acc1[0], image.size(0))\n",
    "            # top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model_inception_resnet = InceptionResnetV1(pretrained='vggface2', classify=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/dataset/imagenet/'\n",
    "saved_model_dir = '/home/avishek/Quantization/model_weights/'\n",
    "float_model_file = 'mobilenet_pretrained_float.pth'\n",
    "scripted_float_model_file = 'mobilenet_quantization_scripted.pth'\n",
    "scripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n",
    "\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "# test_dataset = datasets.ImageFolder('/dataset/VGG_Face2//test_images_aligned')\n",
    "# test_dataset = datasets.ImageFolder('/home/avishek/Quantization/data/test_images_aligned2')\n",
    "# test_dataset.idx_to_class = {i:c for c, i in test_dataset.class_to_idx.items()}\n",
    "# test_loader = DataLoader(test_dataset, collate_fn=collate_fn, num_workers=32)\n",
    "\n",
    "train_dataset = datasets.ImageFolder('/home/avishek/Quantization/data/train_images_aligned2')\n",
    "train_dataset.idx_to_class = {i:c for c, i in train_dataset.class_to_idx.items()}\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_fn, num_workers=32, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.targets[1000:1009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.targets[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligned = []\n",
    "# names = []\n",
    "\n",
    "# for x, y in test_loader:\n",
    "#     # x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "#     # if x_aligned is not None:\n",
    "#     #     print('Face detected with probability: {:8f}'.format(prob))\n",
    "#     aligned.append(transforms.ToTensor()(x))\n",
    "#     names.append(test_loader.dataset.idx_to_class[y])\n",
    "#     # class_id.append(test_loader.dataset.samples[y][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/57237381/runtimeerror-expected-4-dimensional-input-for-4-dimensional-weight-32-3-3-but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_vector = []\n",
    "# names = []\n",
    "\n",
    "# for x, y in test_loader:\n",
    "#     feat_vector.append(model_inception_resnet(transforms.ToTensor()(x).unsqueeze(0)).detach().numpy().reshape(8631, ))\n",
    "#     names.append(test_loader.dataset.idx_to_class[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Block: Before fusion \n",
      "\n",
      " Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "Size (MB): 112.018331\n",
      "\n",
      " Model size before fusion :  None\n",
      "\n",
      " After fusion\n",
      "\n",
      " ConvReLU2d(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (1): ReLU()\n",
      ")\n",
      "Size (MB): 111.714175\n",
      "\n",
      " Model size after fusion :  None\n"
     ]
    }
   ],
   "source": [
    "# print('\\n Block: Before fusion \\n\\n', model_inception_resnet.conv2d_1a.conv)\n",
    "# model_inception_resnet.eval()\n",
    "# print('\\n Model size before fusion : ', print_size_of_model( model_inception_resnet))\n",
    "\n",
    "# # Fuses modules\n",
    "# model_inception_resnet.fuse_model()\n",
    "\n",
    "# # Note fusion of Conv+BN+Relu and Conv+Relu\n",
    "# print('\\n After fusion\\n\\n',model_inception_resnet.conv2d_1a.conv)\n",
    "# # model_inception_resnet.eval()\n",
    "# print('\\n Model size after fusion : ', print_size_of_model( model_inception_resnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1048, 12.8072,  4.7573,  ..., -3.3688, -1.6255, -2.5136]])\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     output = model_inception_resnet(transforms.ToTensor()(image).unsqueeze(0))\n",
    "#     print(output)\n",
    "#     loss = criterion(output, torch.tensor([target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# # input = torch.randn(3, 5, requires_grad=True)\n",
    "# # target = torch.tensor([1, 0, 4])\n",
    "# # output = F.nll_loss(F.log_softmax(input), target)\n",
    "\n",
    "# input = torch.randn(1, 5, requires_grad=True)\n",
    "# target = torch.tensor([1])\n",
    "# output = F.nll_loss(F.log_softmax(input), target)\n",
    "# outputc = criterion(F.log_softmax(input), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 112.019355\n",
      "...................................Evaluation accuracy on 5000 images, 85.71\n"
     ]
    }
   ],
   "source": [
    "num_eval_batches = 100\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(model_inception_resnet)\n",
    "\n",
    "top1, top5 = evaluate(model_inception_resnet, criterion, train_loader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "# torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " After observer insertion \n",
      "\n",
      " ConvReLU2d(\n",
      "  (0): Conv2d(\n",
      "    3, 32, kernel_size=(3, 3), stride=(2, 2)\n",
      "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): ReLU(\n",
      "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/quantization/observer.py:121: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 32\n",
    "\n",
    "# # myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "# # myModel.eval()\n",
    "\n",
    "# Fuse Conv, bn and relu\n",
    "model_inception_resnet.fuse_model()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "model_inception_resnet.qconfig = torch.quantization.default_qconfig\n",
    "print(model_inception_resnet.qconfig)\n",
    "torch.quantization.prepare(model_inception_resnet, inplace=True)\n",
    "\n",
    "# Calibrate first\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "print('\\n After observer insertion \\n\\n', model_inception_resnet.conv2d_1a.conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 112.014939\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(model_inception_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " Block: After fusion and quantization, note fused modules: \n",
      "\n",
      " QuantizedConvReLU2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.05539536103606224, zero_point=0)\n",
      "Size of model after quantization\n",
      "Size (MB): 28.245562\n"
     ]
    }
   ],
   "source": [
    "# Calibrate with the training set\n",
    "evaluate(model_inception_resnet, criterion, train_loader, neval_batches=num_calibration_batches)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "torch.quantization.convert(model_inception_resnet, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Block: After fusion and quantization, note fused modules: \\n\\n',model_inception_resnet.conv2d_1a.conv)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(model_inception_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Overloaded torch operator invoked from Python failed to many any schema:\nquantized::mul() Expected a value of type 'Tensor' for argument 'qb' but instead found type 'float'.\nPosition: 1\nValue: 0.17\nDeclaration: quantized::mul(Tensor qa, Tensor qb, float scale, int zero_point) -> (Tensor qc)\nCast error details: Unable to cast Python instance to C++ type (compile in debug mode for details)\n\nquantized::mul() expected at most 3 argument(s) but received 4 argument(s). Declaration: quantized::mul.out(Tensor qa, Tensor qb, Tensor(a!) out) -> (Tensor(a!) out)\n\nquantized::mul() expected at most 2 argument(s) but received 4 argument(s). Declaration: quantized::mul.Scalar(Tensor qa, Scalar b) -> (Tensor qc)\n\nquantized::mul() expected at most 3 argument(s) but received 4 argument(s). Declaration: quantized::mul.Scalar_out(Tensor qa, Scalar b, Tensor(a!) out) -> (Tensor(a!) out)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8ae51e49d6c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Accuracy of quantized model on validation Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtop1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inception_resnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneval_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_eval_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluation accuracy on %d images, %2.2f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_eval_batches\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-46bd23fe98da>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, data_loader, neval_batches)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# output = model(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inception_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-12297d4c2fda>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_4a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_4b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_6a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-12297d4c2fda>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canservers/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# type: (Tensor, Tensor) -> Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Overloaded torch operator invoked from Python failed to many any schema:\nquantized::mul() Expected a value of type 'Tensor' for argument 'qb' but instead found type 'float'.\nPosition: 1\nValue: 0.17\nDeclaration: quantized::mul(Tensor qa, Tensor qb, float scale, int zero_point) -> (Tensor qc)\nCast error details: Unable to cast Python instance to C++ type (compile in debug mode for details)\n\nquantized::mul() expected at most 3 argument(s) but received 4 argument(s). Declaration: quantized::mul.out(Tensor qa, Tensor qb, Tensor(a!) out) -> (Tensor(a!) out)\n\nquantized::mul() expected at most 2 argument(s) but received 4 argument(s). Declaration: quantized::mul.Scalar(Tensor qa, Scalar b) -> (Tensor qc)\n\nquantized::mul() expected at most 3 argument(s) but received 4 argument(s). Declaration: quantized::mul.Scalar_out(Tensor qa, Scalar b, Tensor(a!) out) -> (Tensor(a!) out)\n\n"
     ]
    }
   ],
   "source": [
    "### Accuracy of quantized model on validation Set\n",
    "top1, top5 = evaluate(model_inception_resnet, criterion, train_loader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ac3bbeb357d502fdd2d0ff5bef79451109bdd8a25a09fbb1f3c81c870d5065"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
